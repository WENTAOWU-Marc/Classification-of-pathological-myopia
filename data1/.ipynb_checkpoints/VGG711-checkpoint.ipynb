{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义数据读取器\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "import visualdl\n",
    "from visualdl import LogWriter\n",
    "from paddle.fluid.dygraph.nn import Conv2D, Pool2D, Linear\n",
    "from paddle.fluid.dygraph.base import to_variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DATADIR = 'PALM-Training400/PALM-Training400'\n",
    "DATADIR2 = 'PALM-Validation400'\n",
    "CSVFILE = 'PALM-Validation-GT/PM_Label_and_Fovea_Location.csv'\n",
    "\n",
    "def transform_img(img):\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.transpose(img, (2,0,1))\n",
    "    img = img.astype('float32')\n",
    "    img = img / 255.\n",
    "    img = img * 2.0 - 1.0\n",
    "    return img\n",
    "\n",
    "\n",
    "def data_loader(datadir, batch_size=5, mode = 'train'):\n",
    "    filenames = os.listdir(datadir)\n",
    "    def reader():\n",
    "        if mode == 'train':\n",
    "            random.shuffle(filenames)\n",
    "        batch_imgs = []\n",
    "        batch_labels = []\n",
    "        for name in filenames:\n",
    "            filepath = os.path.join(datadir, name)\n",
    "            img = cv2.imread(filepath)\n",
    "            img = transform_img(img)\n",
    "            if name[0] == 'H' or name[0] == 'N':\n",
    "                label = 0\n",
    "            elif name[0] == 'P':\n",
    "                label = 1\n",
    "            else:\n",
    "                raise('Not excepted file name')\n",
    "            batch_imgs.append(img)\n",
    "            batch_labels.append(label)\n",
    "            if len(batch_imgs) == batch_size:\n",
    "                imgs_array = np.array(batch_imgs).astype('float32')\n",
    "                labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n",
    "                yield imgs_array, labels_array\n",
    "                batch_imgs = []\n",
    "                batch_labels = []\n",
    "\n",
    "        if len(batch_imgs) > 0:\n",
    "            imgs_array = np.array(batch_imgs).astype('float32')\n",
    "            labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n",
    "            yield imgs_array, labels_array\n",
    "\n",
    "    return reader\n",
    "\n",
    "def valid_data_loader(datadir, csvfile, batch_size=5, mode='valid'):\n",
    "    filelists = open(csvfile).readlines()\n",
    "    def reader():\n",
    "        batch_imgs = []\n",
    "        batch_labels = []\n",
    "        for line in filelists[1:-1]:\n",
    "            line = line.strip().split(',')\n",
    "            name = line[1]\n",
    "#             print('line:')\n",
    "#             print(line)\n",
    "#             print(\"line[2]:\"+line[2])\n",
    "            label = int(line[2])\n",
    "            filepath = os.path.join(datadir, name)\n",
    "            img = cv2.imread(filepath)\n",
    "            img = transform_img(img)\n",
    "            batch_imgs.append(img)\n",
    "            batch_labels.append(label)\n",
    "            if len(batch_imgs) == batch_size:\n",
    "                imgs_array = np.array(batch_imgs).astype('float32')\n",
    "                labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n",
    "                yield imgs_array, labels_array\n",
    "                batch_imgs = []\n",
    "                batch_labels = []\n",
    "\n",
    "        if len(batch_imgs) > 0:\n",
    "            imgs_array = np.array(batch_imgs).astype('float32')\n",
    "            labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n",
    "            yield imgs_array, labels_array\n",
    "\n",
    "    return reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGG(fluid.dygraph.Layer):\n",
    "    def __init__(self):\n",
    "        super(VGG,self).__init__()\n",
    "        self.conv1_1 = Conv2D(num_channels=3,num_filters=64,filter_size=3, padding=1, act='relu')\n",
    "        self.conv1_2 = Conv2D(num_channels=64,num_filters=64,filter_size=3, padding=1, act='relu')\n",
    "        self.pool_1  = Pool2D(pool_stride=2, pool_size = 2, pool_type='max')\n",
    "\n",
    "        self.conv2_1 = Conv2D(num_channels=64,num_filters=128,filter_size=3, padding=1, act='relu')\n",
    "        self.conv2_2 = Conv2D(num_channels=128,num_filters=128,filter_size=3, padding=1, act='relu')\n",
    "        self.pool_2  = Pool2D(pool_stride=2, pool_size = 2, pool_type='max')\n",
    "\n",
    "        self.conv3_1 = Conv2D(num_channels=128,num_filters=256,filter_size=3, padding=1, act='relu')\n",
    "        self.conv3_2 = Conv2D(num_channels=256,num_filters=256,filter_size=3, padding=1, act='relu')\n",
    "        self.conv3_3 = Conv2D(num_channels=256,num_filters=256,filter_size=3, padding=1, act='relu')\n",
    "        self.pool_3  = Pool2D(pool_stride=2, pool_size = 2, pool_type='max')\n",
    "\n",
    "        self.conv4_1 = Conv2D(num_channels=256,num_filters=512,filter_size=3, padding=1, act='relu')\n",
    "        self.conv4_2 = Conv2D(num_channels=512,num_filters=512,filter_size=3, padding=1, act='relu')\n",
    "        self.conv4_3 = Conv2D(num_channels=512,num_filters=512,filter_size=3, padding=1, act='relu')\n",
    "        self.pool_4  = Pool2D(pool_stride=2, pool_size = 2, pool_type='max')\n",
    "\n",
    "        self.conv5_1 = Conv2D(num_channels=512,num_filters=512,filter_size=3, padding=1, act='relu')\n",
    "        self.conv5_2 = Conv2D(num_channels=512,num_filters=512,filter_size=3, padding=1, act='relu')\n",
    "        self.conv5_3 = Conv2D(num_channels=512,num_filters=512,filter_size=3, padding=1, act='relu')\n",
    "        self.pool_5  = Pool2D(pool_stride=2, pool_size = 2, pool_type='max')\n",
    "\n",
    "        self.fc1 = Linear(input_dim=512*7*7, output_dim=4096,act='relu')\n",
    "        self.fc2= Linear(input_dim=4096, output_dim=4096,act='relu')\n",
    "        self.fc3 = Linear(input_dim=4096, output_dim=1)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.pool_2(x)\n",
    "\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.pool_3(x)\n",
    "\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.conv4_3(x)\n",
    "        x = self.pool_4(x)\n",
    "\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.conv5_2(x)\n",
    "        x = self.conv5_3(x)\n",
    "        x = self.pool_5(x)\n",
    "        x = fluid.layers.reshape(x, [x.shape[0], -1])\n",
    "        x = fluid.layers.dropout(self.fc1(x), 0.5)  #0.5是drop的比率\n",
    "        x = fluid.layers.dropout(self.fc2(x), 0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3652548174.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [16]\u001b[0;36m\u001b[0m\n\u001b[0;31m    x.append(epoch)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "with fluid.dygraph.guard():\n",
    "    model = VGG()\n",
    "    print('start training ... ')\n",
    "    model.train()\n",
    "    epoch_num = 21\n",
    "    # 定义优化器\n",
    "    opt = fluid.optimizer.Momentum(learning_rate=0.001, momentum=0.9, parameter_list=model.parameters())\n",
    "    # 定义数据读取器，训练数据读取器和验证数据读取器\n",
    "    train_loader = data_loader(DATADIR, batch_size=5, mode='train')\n",
    "    valid_loader = valid_data_loader(DATADIR2, CSVFILE)\n",
    "    with LogWriter(logdir=\"./log/scalar_test/train\") as writer:\n",
    "        for epoch in range(epoch_num):\n",
    "            x.append(epoch)\n",
    "            train_accuracies = []\n",
    "            train_loss = []\n",
    "            for batch_id, data in enumerate(train_loader()):\n",
    "                x_data, y_data = data\n",
    "                img = fluid.dygraph.to_variable(x_data)\n",
    "                label = fluid.dygraph.to_variable(y_data)\n",
    "                logits = model(img)\n",
    "                pred = fluid.layers.sigmoid(logits)\n",
    "                pred2 = pred * (-1.0) + 1.0\n",
    "                pred = fluid.layers.concat([pred2, pred], axis=1)\n",
    "                \n",
    "                #计算accuracy\n",
    "                acc = fluid.layers.accuracy(pred, fluid.layers.cast(label, dtype='int64'))\n",
    "                # 进行loss计算\n",
    "                loss = fluid.layers.sigmoid_cross_entropy_with_logits(logits, label)  #loss的目的是让sigmoid(logits)去逼近label 所以在预测的时候预测值是sigmoid(logits) \n",
    "                avg_loss = fluid.layers.mean(loss)\n",
    "                train_accuracies.append(acc.numpy())\n",
    "                train_loss.append(loss.numpy())\n",
    "                if batch_id % 10 == 0:\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.numpy()))\n",
    "                # 反向传播，更新权重，清除梯度\n",
    "                avg_loss.backward()\n",
    "                opt.minimize(avg_loss)\n",
    "                model.clear_gradients()\n",
    "            train_loss_list.append(np.mean(train_loss))\n",
    "            train_acc_list.append(np.mean(train_accuracies)) \n",
    "            \n",
    "            model.eval()\n",
    "            accuracies = []\n",
    "            losses = []\n",
    "            for batch_id, data in enumerate(valid_loader()):\n",
    "                x_data, y_data = data\n",
    "                img = fluid.dygraph.to_variable(x_data)\n",
    "                label = fluid.dygraph.to_variable(y_data)\n",
    "                # 运行模型前向计算，得到预测值\n",
    "                logits = model(img)\n",
    "                # 二分类，sigmoid计算后的结果以0.5为阈值分两个类别\n",
    "                # 计算sigmoid后的预测概率，进行loss计算\n",
    "                pred = fluid.layers.sigmoid(logits)## 这个值大余）0.5就代表预测值为1\n",
    "                loss = fluid.layers.sigmoid_cross_entropy_with_logits(logits, label)\n",
    "                pred2 = pred * (-1.0) + 1.0\n",
    "                # 得到两个类别的预测概率，并沿第一个维度级联\n",
    "                pred = fluid.layers.concat([pred2, pred], axis=1) # [10，2]\n",
    "                acc = fluid.layers.accuracy(pred, fluid.layers.cast(label, dtype='int64'))\n",
    "                accuracies.append(acc.numpy())\n",
    "                losses.append(loss.numpy())\n",
    "            val_loss_list.append(np.mean(losses))\n",
    "            val_acc_list.append(np.mean(accuracies))\n",
    "            print(\"[validation] accuracy/loss: {}/{}\".format(np.mean(accuracies), np.mean(losses)))\n",
    "            writer.add_scalar(tag=\"acc\",step=epoch,value = np.mean(accuracies))\n",
    "            writer.add_scalar(tag=\"loss\",step=epoch,value = np.mean(losses))\n",
    "            model.train()\n",
    "            plt.figure(figsize=(3, 6), dpi=100)\n",
    "            plt.subplots_adjust(hspace=0.5)  # 添加这一行来调整子图间距\n",
    "            # 创建两行一列的图，并指定当前使用第一个图\n",
    "            plt.subplot(2, 1, 1)\n",
    "            try:\n",
    "                train_loss_lines.remove(train_loss_lines[0])  # 移除上一步曲线\n",
    "                val_loss_lines.remove(val_loss_lines[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "            train_loss_lines = plt.plot(x, train_loss_list, 'r', lw=1)  # lw为曲线宽度\n",
    "            val_loss_lines = plt.plot(x, val_loss_list, 'b', lw=1)\n",
    "            plt.title(\"loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.legend([\"train_loss\",\n",
    "                \"val_loss\"])\n",
    "\n",
    "            # 创建两行一列的图，并指定当前使用第二个图\n",
    "            plt.subplot(2, 1, 2)\n",
    "            try:\n",
    "                train_acc_lines.remove(train_acc_lines[0])  # 移除上一步曲线\n",
    "                val_acc_lines.remove(val_acc_lines[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            train_acc_lines = plt.plot(x, train_acc_list, 'r', lw=1)  # lw为曲线宽度\n",
    "            val_acc_lines = plt.plot(x, val_acc_list, 'b', lw=1)\n",
    "            plt.title(\"acc\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.ylabel(\"acc\")\n",
    "            plt.legend([\"train_acc\",\n",
    "                        \"val_acc\"])\n",
    "\n",
    "            plt.show()\n",
    "            plt.pause(0.1)  # 图片停留0.1s\n",
    "    # save params of model\n",
    "    fluid.save_dygraph(model.state_dict(), 'palm')\n",
    "    # save optimizer state\n",
    "    fluid.save_dygraph(opt.state_dict(), 'palm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualdl.server import app\n",
    "app.run(\"./log/scalar_test/train\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=8081,\n",
    "        cache_timeout=20,            \n",
    "        language=None,            \n",
    "        public_path=None,    \n",
    "        api_only=False,             \n",
    "        open_browser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = valid_data_loader(DATADIR2, CSVFILE)\n",
    "model.eval()\n",
    "accuracies = []\n",
    "losses = []\n",
    "with fluid.dygraph.guard():\n",
    "    for batch_id, data in enumerate(valid_loader()):\n",
    "        x_data, y_data = data\n",
    "        img = fluid.dygraph.to_variable(x_data)\n",
    "        label = fluid.dygraph.to_variable(y_data)\n",
    "        # 运行模型前向计算，得到预测值\n",
    "        logits,conv = model(img)\n",
    "        pred = fluid.layers.sigmoid(logits)\n",
    "        loss = fluid.layers.sigmoid_cross_entropy_with_logits(logits, label)\n",
    "        avg_loss = fluid.layers.mean(loss)\n",
    "        # 计算预测概率小于0.5的类别\n",
    "        pred2 = pred * (-1.0) + 1.0\n",
    "        # 得到两个类别的预测概率，并沿第一个维度级联\n",
    "        pred = fluid.layers.concat([pred2, pred], axis=1)\n",
    "        acc = fluid.layers.accuracy(pred, fluid.layers.cast(label, dtype='int64'))\n",
    "        accuracies.append(acc.numpy())\n",
    "        losses.append(loss.numpy())\n",
    "print(\"[validation] accuracy/loss: {}/{}\".format(np.mean(accuracies), np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
