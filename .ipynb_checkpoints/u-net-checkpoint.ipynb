{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f8ddbd-6fdd-470e-bfae-647246372a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OMP_NUM_THREADS set to 14, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from numpy.core.defchararray import decode, mod\n",
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.fluid as fluid\n",
    "from paddle.fluid.dygraph import to_variable\n",
    "from paddle.fluid.dygraph import Layer\n",
    "from paddle.fluid.dygraph import Conv2D\n",
    "from paddle.fluid.dygraph import BatchNorm\n",
    "from paddle.fluid.dygraph import Pool2D\n",
    "from paddle.fluid.dygraph import Conv2DTranspose\n",
    "from visualdl import LogWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc55e2c4-a80d-4941-b981-f74c2d3b64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, num_channels, num_filters):\n",
    "        super(Encoder, self).__init__()\n",
    "        # TODO：encoder contains:\n",
    "        # 3×3 conv + bn + relu\n",
    "        # 3×3 conv + bn + relu\n",
    "        # 2×2 pool\n",
    "        # return features before and after pool  \n",
    "        self.conv1 = Conv2D(num_channels=num_channels,\n",
    "                            num_filters=num_filters,\n",
    "                            filter_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1)  # 3×3卷积的时候，padding=1的时候，尺寸不会变\n",
    "        self.bn1 = BatchNorm(num_filters, act='relu')\n",
    "\n",
    "        self.conv2 = Conv2D(num_channels=num_filters,\n",
    "                            num_filters=num_filters,\n",
    "                            filter_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1)\n",
    "        self.bn2 = BatchNorm(num_filters, act='relu')\n",
    "\n",
    "        self.pool = Pool2D(pool_size=2, pool_stride=2, pool_type='max', ceil_mode=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x) # 灰色箭头concat\n",
    "        x_pooled = self.pool(x)\n",
    "        \n",
    "        return x, x_pooled\n",
    "\n",
    "\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, num_channels, num_filters):\n",
    "        super(Decoder, self).__init__()\n",
    "        # TODO：encoder contains:\n",
    "        # 2×2 transpose conv, stride=2, p=0 (makes feature map 2× larger)\n",
    "        # 3×3 conv + bn + relu\n",
    "        # 3×3 conv + bn + relu\n",
    "        self.up = Conv2DTranspose(num_channels=num_channels,    # 1024->512\n",
    "                                  num_filters=num_filters,\n",
    "                                  filter_size=2,\n",
    "                                  stride=2)\n",
    "        \n",
    "        self.conv1 = Conv2D(num_channels=num_channels,  # 1024\n",
    "                            num_filters=num_filters,\n",
    "                            filter_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1)\n",
    "\n",
    "        self.bn1 = BatchNorm(num_channels=num_filters, act='relu')\n",
    "\n",
    "        self.conv2 = Conv2D(num_channels=num_filters,\n",
    "                            num_filters=num_filters,\n",
    "                            filter_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1)\n",
    "\n",
    "        self.bn2 = BatchNorm(num_channels=num_filters, act='relu')\n",
    "    \n",
    "    def forward(self, inputs_prev, inputs):\n",
    "        # TODO:forward contains an pad2d and concat\n",
    "        # 原论文是input_prev进行crop，这里是对x进行padding，目的一样，就是把保证HW一致，进行concat\n",
    "        x = self.up(inputs)\n",
    "        # NCHW\n",
    "        h_diff = (inputs_prev.shape[2] - x.shape[2])\n",
    "        w_diff = (inputs_prev.shape[3] - x.shape[3])\n",
    "        x = fluid.layers.pad2d(x, paddings=[h_diff//2, h_diff - h_diff//2, w_diff//2, w_diff - w_diff//2])\n",
    "        # axis=1为C。NCHW，把channel concat\n",
    "        x = fluid.layers.concat([inputs_prev, x], axis=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(Layer):\n",
    "    def __init__(self, num_classes=59):\n",
    "        super(UNet, self).__init__()\n",
    "        # encoder: 3->64->128->256->512\n",
    "        # mid: 512->1024->1024\n",
    "\n",
    "        # TODO: 4 encoders, 4 decoders, and mid layers contain 2x (1x1conv+bn+relu)\n",
    "        self.down1 = Encoder(num_channels=3, num_filters=64)\n",
    "        self.down2 = Encoder(num_channels=64, num_filters=128)\n",
    "        self.down3 = Encoder(num_channels=128, num_filters=256)\n",
    "        self.down4 = Encoder(num_channels=256, num_filters=512)\n",
    "\n",
    "        # 原论文应该是 3x3 padding=1,stride=1,这里使用1x1卷积\n",
    "        self.midconv1 = Conv2D(num_channels=512, num_filters=1024, filter_size=1, padding =0, stride=1)\n",
    "        self.bn1 = BatchNorm(num_channels=1024, act='relu')\n",
    "        self.midconv2 = Conv2D(num_channels=1024, num_filters=1024, filter_size=1, padding=0, stride=1)\n",
    "        self.bn2 = BatchNorm(num_channels=1024, act='relu')\n",
    "\n",
    "        self.up1 = Decoder(num_channels=1024, num_filters=512)\n",
    "        self.up2 = Decoder(num_channels=512, num_filters=256)\n",
    "        self.up3 = Decoder(num_channels=256, num_filters=128)\n",
    "        self.up4 = Decoder(num_channels=128, num_filters=64)\n",
    "\n",
    "        # last_conv： channel：64->num_classes\n",
    "        self.last_conv = Conv2D(num_channels=64, num_filters=num_classes, filter_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # encoder layer\n",
    "        #print('encoder layer:')\n",
    "        x1, x = self.down1(inputs)\n",
    "        #print('input_pred:',x1.shape, 'x_pooled：', x.shape)\n",
    "        x2, x = self.down2(x)\n",
    "        #print('input_pred:',x2.shape, 'x_pooled：', x.shape)\n",
    "        x3, x = self.down3(x)\n",
    "        #print('input_pred:',x3.shape, 'x_pooled：', x.shape)\n",
    "        x4, x = self.down4(x)\n",
    "        #print('input_pred:',x4.shape, 'x_pooled：', x.shape)\n",
    "\n",
    "        # middle layer\n",
    "        x = self.midconv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.midconv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        # decoder layer\n",
    "        #print('decoder layer:')\n",
    "        x = self.up1(x4, x)\n",
    "        #print('up1_input_pred:',x4.shape, 'up1：', x.shape)\n",
    "        x = self.up2(x3, x)\n",
    "        #print('up2_input_pred:',x3.shape, 'up2：', x.shape)\n",
    "        x = self.up3(x2, x)\n",
    "        #print('up3_input_pred:',x2.shape, 'up3：', x.shape)\n",
    "        x = self.up4(x1, x)\n",
    "        #print('up4_input_pred:',x1.shape, 'up4：', x.shape)\n",
    "\n",
    "        x = self.last_conv(x)\n",
    "        #print('out_put:', x.shape)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cf6b8c-d45e-4ed3-9e86-b7231d465cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据加载\n",
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform_size=(512, 512)):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform_size = transform_size\n",
    "        self.image_list = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_list[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, os.path.splitext(self.image_list[idx])[0] + '.bmp')\n",
    "        \n",
    "        # 使用resize方法调整图像和掩码的尺寸\n",
    "        image = Image.open(image_path).convert('RGB').resize(self.transform_size)\n",
    "        mask = Image.open(mask_path).convert('L').resize(self.transform_size)\n",
    "        \n",
    "        image = paddle.to_tensor(np.array(image).astype('float32').transpose((2, 0, 1)) / 255.0)\n",
    "        mask = paddle.to_tensor(np.array(mask).astype('float32')[np.newaxis, :, :] / 255.0)\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "data_dir = \"data/Train\"\n",
    "fundus_image_dir = os.path.join(data_dir, \"fundus_image\")\n",
    "completed_masks_dir = os.path.join(data_dir, \"Lesion_Masks\", \"Completed_Masks\", \"Atrophy\")  # 使用Atrophy作为示例\n",
    "\n",
    "train_dataset = EyeDataset(fundus_image_dir, completed_masks_dir, transform_size=(512,512))\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2679cc9-0579-48d6-9dc9-4298e41301a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 14:46:12.950276   815 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2\n",
      "W0813 14:46:12.954282   815 device_context.cc:465] device: 0, cuDNN Version: 8.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Batch 1/100, Loss: 0.8464601039886475\n",
      "Epoch 1/50, Batch 2/100, Loss: 0.7884594202041626\n",
      "Epoch 1/50, Batch 3/100, Loss: 0.5653164386749268\n",
      "Epoch 1/50, Batch 4/100, Loss: 0.44434690475463867\n",
      "Epoch 1/50, Batch 5/100, Loss: 0.3931688964366913\n",
      "Epoch 1/50, Batch 6/100, Loss: 0.3720991909503937\n",
      "Epoch 1/50, Batch 7/100, Loss: 0.33289018273353577\n",
      "Epoch 1/50, Batch 8/100, Loss: 0.32711198925971985\n",
      "Epoch 1/50, Batch 9/100, Loss: 0.3097726106643677\n",
      "Epoch 1/50, Batch 10/100, Loss: 0.28116437792778015\n",
      "Epoch 1/50, Batch 11/100, Loss: 0.217435821890831\n",
      "Epoch 1/50, Batch 12/100, Loss: 0.3669527769088745\n",
      "Epoch 1/50, Batch 13/100, Loss: 0.21827994287014008\n",
      "Epoch 1/50, Batch 14/100, Loss: 0.2626889646053314\n",
      "Epoch 1/50, Batch 15/100, Loss: 0.2929624617099762\n",
      "Epoch 1/50, Batch 16/100, Loss: 0.2765800356864929\n",
      "Epoch 1/50, Batch 17/100, Loss: 0.22495310008525848\n",
      "Epoch 1/50, Batch 18/100, Loss: 0.317893922328949\n",
      "Epoch 1/50, Batch 19/100, Loss: 0.16910454630851746\n",
      "Epoch 1/50, Batch 20/100, Loss: 0.21576982736587524\n",
      "Epoch 1/50, Batch 21/100, Loss: 0.33354651927948\n",
      "Epoch 1/50, Batch 22/100, Loss: 0.2324659377336502\n",
      "Epoch 1/50, Batch 23/100, Loss: 0.242126926779747\n",
      "Epoch 1/50, Batch 24/100, Loss: 0.3474171757698059\n",
      "Epoch 1/50, Batch 25/100, Loss: 0.18805193901062012\n",
      "Epoch 1/50, Batch 26/100, Loss: 0.18284332752227783\n",
      "Epoch 1/50, Batch 27/100, Loss: 0.2593589127063751\n",
      "Epoch 1/50, Batch 28/100, Loss: 0.2861727774143219\n",
      "Epoch 1/50, Batch 29/100, Loss: 0.19119134545326233\n",
      "Epoch 1/50, Batch 30/100, Loss: 0.24234238266944885\n",
      "Epoch 1/50, Batch 31/100, Loss: 0.28691184520721436\n",
      "Epoch 1/50, Batch 32/100, Loss: 0.219255730509758\n",
      "Epoch 1/50, Batch 33/100, Loss: 0.24584977328777313\n",
      "Epoch 1/50, Batch 34/100, Loss: 0.2550392150878906\n",
      "Epoch 1/50, Batch 35/100, Loss: 0.1970338523387909\n",
      "Epoch 1/50, Batch 36/100, Loss: 0.18201902508735657\n",
      "Epoch 1/50, Batch 37/100, Loss: 0.20959900319576263\n",
      "Epoch 1/50, Batch 38/100, Loss: 0.18192413449287415\n",
      "Epoch 1/50, Batch 39/100, Loss: 0.267483651638031\n",
      "Epoch 1/50, Batch 40/100, Loss: 0.20010101795196533\n",
      "Epoch 1/50, Batch 41/100, Loss: 0.1961778998374939\n",
      "Epoch 1/50, Batch 42/100, Loss: 0.21074263751506805\n",
      "Epoch 1/50, Batch 43/100, Loss: 0.3530125617980957\n",
      "Epoch 1/50, Batch 44/100, Loss: 0.25614213943481445\n",
      "Epoch 1/50, Batch 45/100, Loss: 0.17047718167304993\n",
      "Epoch 1/50, Batch 46/100, Loss: 0.1663929671049118\n",
      "Epoch 1/50, Batch 47/100, Loss: 0.17738786339759827\n",
      "Epoch 1/50, Batch 48/100, Loss: 0.17663347721099854\n",
      "Epoch 1/50, Batch 49/100, Loss: 0.19518044590950012\n",
      "Epoch 1/50, Batch 50/100, Loss: 0.20411524176597595\n",
      "Epoch 1/50, Batch 51/100, Loss: 0.21154695749282837\n",
      "Epoch 1/50, Batch 52/100, Loss: 0.2750632166862488\n",
      "Epoch 1/50, Batch 53/100, Loss: 0.16480174660682678\n",
      "Epoch 1/50, Batch 54/100, Loss: 0.16299329698085785\n",
      "Epoch 1/50, Batch 55/100, Loss: 0.2910645604133606\n",
      "Epoch 1/50, Batch 56/100, Loss: 0.2090604305267334\n",
      "Epoch 1/50, Batch 57/100, Loss: 0.23744963109493256\n",
      "Epoch 1/50, Batch 58/100, Loss: 0.26144060492515564\n"
     ]
    }
   ],
   "source": [
    "import time  # 导入time模块\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# 创建一个LogWriter对象\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "log_dir = f\"unet_vdl_logs/{current_time}_epochs_{epochs}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "writer = LogWriter(log_dir)\n",
    "\n",
    "# 3. 模型训练\n",
    "model = UNet(num_classes=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(parameters=model.parameters(), learning_rate=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()  # 记录epoch开始时间\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        total_loss += loss.numpy()[0]\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.numpy()[0]}\")\n",
    "        \n",
    "    # 记录平均loss到VisualDL\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar(tag=\"train/avg_loss\", step=epoch, value=avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss}\")\n",
    "\n",
    "    # 每个epoch结束后保存模型\n",
    "    model_path = os.path.join(\"Unet_models\", f\"unet_epoch{epoch+1}.pdparams\")\n",
    "    paddle.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model parameters to {model_path}\")\n",
    "\n",
    "    end_time = time.time()  # 记录epoch结束时间\n",
    "    elapsed_time = end_time - start_time  # 计算epoch的训练时间\n",
    "    print(f\"Epoch {epoch+1} training time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca246c0-9cd7-49e4-b551-832dc836df37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55884472-89d1-4c98-901f-57f18c9db5a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 19:58:10.036633   816 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.0, Runtime API Version: 11.2\n",
      "W0812 19:58:10.040460   816 device_context.cc:465] device: 0, cuDNN Version: 8.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model parameters from models/unet_epoch50.pdparams\n",
      "Epoch 51/60, Batch 1/100, Loss: 0.06657276302576065\n",
      "Epoch 51/60, Batch 2/100, Loss: 0.0694594755768776\n",
      "Epoch 51/60, Batch 3/100, Loss: 0.15546706318855286\n",
      "Epoch 51/60, Batch 4/100, Loss: 0.07975435256958008\n",
      "Epoch 51/60, Batch 5/100, Loss: 0.07151706516742706\n",
      "Epoch 51/60, Batch 6/100, Loss: 0.08240658789873123\n",
      "Epoch 51/60, Batch 7/100, Loss: 0.04608643054962158\n",
      "Epoch 51/60, Batch 8/100, Loss: 0.06681846082210541\n",
      "Epoch 51/60, Batch 9/100, Loss: 0.04484133422374725\n",
      "Epoch 51/60, Batch 10/100, Loss: 0.15420924127101898\n",
      "Epoch 51/60, Batch 11/100, Loss: 0.09321074187755585\n",
      "Epoch 51/60, Batch 12/100, Loss: 0.04096284508705139\n",
      "Epoch 51/60, Batch 13/100, Loss: 0.05036269873380661\n",
      "Epoch 51/60, Batch 14/100, Loss: 0.06247153505682945\n",
      "Epoch 51/60, Batch 15/100, Loss: 0.06125855818390846\n",
      "Epoch 51/60, Batch 16/100, Loss: 0.03740565478801727\n",
      "Epoch 51/60, Batch 17/100, Loss: 0.06555814296007156\n",
      "Epoch 51/60, Batch 18/100, Loss: 0.05095059052109718\n",
      "Epoch 51/60, Batch 19/100, Loss: 0.046226684004068375\n",
      "Epoch 51/60, Batch 20/100, Loss: 0.030021926388144493\n",
      "Epoch 51/60, Batch 21/100, Loss: 0.03171490132808685\n",
      "Epoch 51/60, Batch 22/100, Loss: 0.10076936334371567\n",
      "Epoch 51/60, Batch 23/100, Loss: 0.06950370222330093\n",
      "Epoch 51/60, Batch 24/100, Loss: 0.14734528958797455\n",
      "Epoch 51/60, Batch 25/100, Loss: 0.15420174598693848\n",
      "Epoch 51/60, Batch 26/100, Loss: 0.031175488606095314\n",
      "Epoch 51/60, Batch 27/100, Loss: 0.044097207486629486\n",
      "Epoch 51/60, Batch 28/100, Loss: 0.0955837294459343\n",
      "Epoch 51/60, Batch 29/100, Loss: 0.04519852250814438\n",
      "Epoch 51/60, Batch 30/100, Loss: 0.03491535037755966\n",
      "Epoch 51/60, Batch 31/100, Loss: 0.2088925540447235\n",
      "Epoch 51/60, Batch 32/100, Loss: 0.03283121809363365\n",
      "Epoch 51/60, Batch 33/100, Loss: 0.09709607809782028\n",
      "Epoch 51/60, Batch 34/100, Loss: 0.017885146662592888\n",
      "Epoch 51/60, Batch 35/100, Loss: 0.07802140712738037\n",
      "Epoch 51/60, Batch 36/100, Loss: 0.030794944614171982\n",
      "Epoch 51/60, Batch 37/100, Loss: 0.03840605169534683\n",
      "Epoch 51/60, Batch 38/100, Loss: 0.07714536041021347\n",
      "Epoch 51/60, Batch 39/100, Loss: 0.1437443494796753\n",
      "Epoch 51/60, Batch 40/100, Loss: 0.026506833732128143\n",
      "Epoch 51/60, Batch 41/100, Loss: 0.05423727631568909\n",
      "Epoch 51/60, Batch 42/100, Loss: 0.07813145965337753\n",
      "Epoch 51/60, Batch 43/100, Loss: 0.0312117300927639\n",
      "Epoch 51/60, Batch 44/100, Loss: 0.05923721566796303\n",
      "Epoch 51/60, Batch 45/100, Loss: 0.06345267593860626\n",
      "Epoch 51/60, Batch 46/100, Loss: 0.07207345217466354\n",
      "Epoch 51/60, Batch 47/100, Loss: 0.12260523438453674\n",
      "Epoch 51/60, Batch 48/100, Loss: 0.06678729504346848\n",
      "Epoch 51/60, Batch 49/100, Loss: 0.06323203444480896\n",
      "Epoch 51/60, Batch 50/100, Loss: 0.03273972123861313\n",
      "Epoch 51/60, Batch 51/100, Loss: 0.07927781343460083\n",
      "Epoch 51/60, Batch 52/100, Loss: 0.061451058834791183\n",
      "Epoch 51/60, Batch 53/100, Loss: 0.05416754260659218\n",
      "Epoch 51/60, Batch 54/100, Loss: 0.04637734964489937\n",
      "Epoch 51/60, Batch 55/100, Loss: 0.046876683831214905\n",
      "Epoch 51/60, Batch 56/100, Loss: 0.041811347007751465\n",
      "Epoch 51/60, Batch 57/100, Loss: 0.02301066368818283\n",
      "Epoch 51/60, Batch 58/100, Loss: 0.017844099551439285\n",
      "Epoch 51/60, Batch 59/100, Loss: 0.19078627228736877\n",
      "Epoch 51/60, Batch 60/100, Loss: 0.05584300309419632\n",
      "Epoch 51/60, Batch 61/100, Loss: 0.03934628143906593\n",
      "Epoch 51/60, Batch 62/100, Loss: 0.06699874252080917\n",
      "Epoch 51/60, Batch 63/100, Loss: 0.05888056010007858\n",
      "Epoch 51/60, Batch 64/100, Loss: 0.0497271865606308\n",
      "Epoch 51/60, Batch 65/100, Loss: 0.027916407212615013\n",
      "Epoch 51/60, Batch 66/100, Loss: 0.05708529055118561\n",
      "Epoch 51/60, Batch 67/100, Loss: 0.06650201231241226\n",
      "Epoch 51/60, Batch 68/100, Loss: 0.0878264307975769\n",
      "Epoch 51/60, Batch 69/100, Loss: 0.025889575481414795\n",
      "Epoch 51/60, Batch 70/100, Loss: 0.14813359081745148\n",
      "Epoch 51/60, Batch 71/100, Loss: 0.09226556867361069\n",
      "Epoch 51/60, Batch 72/100, Loss: 0.07833206653594971\n",
      "Epoch 51/60, Batch 73/100, Loss: 0.06357982009649277\n",
      "Epoch 51/60, Batch 74/100, Loss: 0.06586142629384995\n",
      "Epoch 51/60, Batch 75/100, Loss: 0.048334989696741104\n",
      "Epoch 51/60, Batch 76/100, Loss: 0.04396975785493851\n",
      "Epoch 51/60, Batch 77/100, Loss: 0.06733386218547821\n",
      "Epoch 51/60, Batch 78/100, Loss: 0.07953663170337677\n",
      "Epoch 51/60, Batch 79/100, Loss: 0.05454782396554947\n",
      "Epoch 51/60, Batch 80/100, Loss: 0.10853303968906403\n",
      "Epoch 51/60, Batch 81/100, Loss: 0.08039166033267975\n",
      "Epoch 51/60, Batch 82/100, Loss: 0.06114362180233002\n",
      "Epoch 51/60, Batch 83/100, Loss: 0.028673337772488594\n",
      "Epoch 51/60, Batch 84/100, Loss: 0.0802692174911499\n",
      "Epoch 51/60, Batch 85/100, Loss: 0.070797860622406\n",
      "Epoch 51/60, Batch 86/100, Loss: 0.04926352575421333\n",
      "Epoch 51/60, Batch 87/100, Loss: 0.027300555258989334\n",
      "Epoch 51/60, Batch 88/100, Loss: 0.1071135625243187\n",
      "Epoch 51/60, Batch 89/100, Loss: 0.05960400402545929\n",
      "Epoch 51/60, Batch 90/100, Loss: 0.07125617563724518\n",
      "Epoch 51/60, Batch 91/100, Loss: 0.0646369531750679\n",
      "Epoch 51/60, Batch 92/100, Loss: 0.11704715341329575\n",
      "Epoch 51/60, Batch 93/100, Loss: 0.010757039301097393\n",
      "Epoch 51/60, Batch 94/100, Loss: 0.07109047472476959\n",
      "Epoch 51/60, Batch 95/100, Loss: 0.03391779959201813\n",
      "Epoch 51/60, Batch 96/100, Loss: 0.034599583595991135\n",
      "Epoch 51/60, Batch 97/100, Loss: 0.028863677754998207\n",
      "Epoch 51/60, Batch 98/100, Loss: 0.05684757977724075\n",
      "Epoch 51/60, Batch 99/100, Loss: 0.18428340554237366\n",
      "Epoch 51/60, Batch 100/100, Loss: 0.10981494933366776\n",
      "Saved model parameters to models/unet_epoch51.pdparams\n",
      "Epoch 52/60, Batch 1/100, Loss: 0.07794829457998276\n",
      "Epoch 52/60, Batch 2/100, Loss: 0.050252266228199005\n",
      "Epoch 52/60, Batch 3/100, Loss: 0.03763340413570404\n",
      "Epoch 52/60, Batch 4/100, Loss: 0.07461734116077423\n",
      "Epoch 52/60, Batch 5/100, Loss: 0.04965471848845482\n",
      "Epoch 52/60, Batch 6/100, Loss: 0.013724652118980885\n",
      "Epoch 52/60, Batch 7/100, Loss: 0.04530389979481697\n",
      "Epoch 52/60, Batch 8/100, Loss: 0.051988422870635986\n",
      "Epoch 52/60, Batch 9/100, Loss: 0.09551577270030975\n",
      "Epoch 52/60, Batch 10/100, Loss: 0.05638742819428444\n",
      "Epoch 52/60, Batch 11/100, Loss: 0.03488977253437042\n",
      "Epoch 52/60, Batch 12/100, Loss: 0.03553633764386177\n",
      "Epoch 52/60, Batch 13/100, Loss: 0.06271971762180328\n",
      "Epoch 52/60, Batch 14/100, Loss: 0.042737893760204315\n",
      "Epoch 52/60, Batch 15/100, Loss: 0.04182559624314308\n",
      "Epoch 52/60, Batch 16/100, Loss: 0.03695051744580269\n",
      "Epoch 52/60, Batch 17/100, Loss: 0.02989138476550579\n",
      "Epoch 52/60, Batch 18/100, Loss: 0.10965552181005478\n",
      "Epoch 52/60, Batch 19/100, Loss: 0.028725363314151764\n",
      "Epoch 52/60, Batch 20/100, Loss: 0.04578816890716553\n",
      "Epoch 52/60, Batch 21/100, Loss: 0.07285497337579727\n",
      "Epoch 52/60, Batch 22/100, Loss: 0.04361964762210846\n",
      "Epoch 52/60, Batch 23/100, Loss: 0.019906016066670418\n",
      "Epoch 52/60, Batch 24/100, Loss: 0.057510145008563995\n",
      "Epoch 52/60, Batch 25/100, Loss: 0.09469539672136307\n",
      "Epoch 52/60, Batch 26/100, Loss: 0.033547237515449524\n",
      "Epoch 52/60, Batch 27/100, Loss: 0.03270180523395538\n",
      "Epoch 52/60, Batch 28/100, Loss: 0.06759002059698105\n",
      "Epoch 52/60, Batch 29/100, Loss: 0.048464398831129074\n",
      "Epoch 52/60, Batch 30/100, Loss: 0.0172591470181942\n",
      "Epoch 52/60, Batch 31/100, Loss: 0.050303809344768524\n",
      "Epoch 52/60, Batch 32/100, Loss: 0.023698698729276657\n",
      "Epoch 52/60, Batch 33/100, Loss: 0.0314023420214653\n",
      "Epoch 52/60, Batch 34/100, Loss: 0.03428211435675621\n",
      "Epoch 52/60, Batch 35/100, Loss: 0.06768250465393066\n",
      "Epoch 52/60, Batch 36/100, Loss: 0.041317153722047806\n",
      "Epoch 52/60, Batch 37/100, Loss: 0.23985539376735687\n",
      "Epoch 52/60, Batch 38/100, Loss: 0.1686762422323227\n",
      "Epoch 52/60, Batch 39/100, Loss: 0.0459257997572422\n",
      "Epoch 52/60, Batch 40/100, Loss: 0.0270131453871727\n",
      "Epoch 52/60, Batch 41/100, Loss: 0.12325379252433777\n",
      "Epoch 52/60, Batch 42/100, Loss: 0.04847985878586769\n",
      "Epoch 52/60, Batch 43/100, Loss: 0.16974732279777527\n",
      "Epoch 52/60, Batch 44/100, Loss: 0.0814291313290596\n",
      "Epoch 52/60, Batch 45/100, Loss: 0.07967070490121841\n",
      "Epoch 52/60, Batch 46/100, Loss: 0.05566577613353729\n",
      "Epoch 52/60, Batch 47/100, Loss: 0.056065335869789124\n",
      "Epoch 52/60, Batch 48/100, Loss: 0.06442774832248688\n",
      "Epoch 52/60, Batch 49/100, Loss: 0.06461362540721893\n",
      "Epoch 52/60, Batch 50/100, Loss: 0.032136764377355576\n",
      "Epoch 52/60, Batch 51/100, Loss: 0.08882397413253784\n",
      "Epoch 52/60, Batch 52/100, Loss: 0.05896224454045296\n",
      "Epoch 52/60, Batch 53/100, Loss: 0.056475620716810226\n",
      "Epoch 52/60, Batch 54/100, Loss: 0.02207520604133606\n",
      "Epoch 52/60, Batch 55/100, Loss: 0.023506203666329384\n",
      "Epoch 52/60, Batch 56/100, Loss: 0.020518910139799118\n",
      "Epoch 52/60, Batch 57/100, Loss: 0.06265096366405487\n",
      "Epoch 52/60, Batch 58/100, Loss: 0.03352084755897522\n",
      "Epoch 52/60, Batch 59/100, Loss: 0.014105617068707943\n",
      "Epoch 52/60, Batch 60/100, Loss: 0.06380988657474518\n",
      "Epoch 52/60, Batch 61/100, Loss: 0.16508537530899048\n",
      "Epoch 52/60, Batch 62/100, Loss: 0.08206769078969955\n",
      "Epoch 52/60, Batch 63/100, Loss: 0.06146135553717613\n",
      "Epoch 52/60, Batch 64/100, Loss: 0.17183372378349304\n",
      "Epoch 52/60, Batch 65/100, Loss: 0.1572982370853424\n",
      "Epoch 52/60, Batch 66/100, Loss: 0.046383827924728394\n",
      "Epoch 52/60, Batch 67/100, Loss: 0.05644124746322632\n",
      "Epoch 52/60, Batch 68/100, Loss: 0.04799331724643707\n",
      "Epoch 52/60, Batch 69/100, Loss: 0.031741682440042496\n",
      "Epoch 52/60, Batch 70/100, Loss: 0.029098473489284515\n",
      "Epoch 52/60, Batch 71/100, Loss: 0.06798554211854935\n",
      "Epoch 52/60, Batch 72/100, Loss: 0.10875646770000458\n",
      "Epoch 52/60, Batch 73/100, Loss: 0.0671929344534874\n",
      "Epoch 52/60, Batch 74/100, Loss: 0.07848594337701797\n",
      "Epoch 52/60, Batch 75/100, Loss: 0.03479496389627457\n",
      "Epoch 52/60, Batch 76/100, Loss: 0.1648155152797699\n",
      "Epoch 52/60, Batch 77/100, Loss: 0.036167994141578674\n",
      "Epoch 52/60, Batch 78/100, Loss: 0.05298449471592903\n",
      "Epoch 52/60, Batch 79/100, Loss: 0.10164429247379303\n",
      "Epoch 52/60, Batch 80/100, Loss: 0.06684626638889313\n",
      "Epoch 52/60, Batch 81/100, Loss: 0.023391075432300568\n",
      "Epoch 52/60, Batch 82/100, Loss: 0.044742196798324585\n",
      "Epoch 52/60, Batch 83/100, Loss: 0.025318950414657593\n",
      "Epoch 52/60, Batch 84/100, Loss: 0.025567442178726196\n",
      "Epoch 52/60, Batch 85/100, Loss: 0.0665379986166954\n",
      "Epoch 52/60, Batch 86/100, Loss: 0.16702255606651306\n",
      "Epoch 52/60, Batch 87/100, Loss: 0.02619190514087677\n",
      "Epoch 52/60, Batch 88/100, Loss: 0.05105336010456085\n",
      "Epoch 52/60, Batch 89/100, Loss: 0.050936587154865265\n",
      "Epoch 52/60, Batch 90/100, Loss: 0.037696678191423416\n",
      "Epoch 52/60, Batch 91/100, Loss: 0.07868631184101105\n",
      "Epoch 52/60, Batch 92/100, Loss: 0.11442446708679199\n",
      "Epoch 52/60, Batch 93/100, Loss: 0.06727837026119232\n",
      "Epoch 52/60, Batch 94/100, Loss: 0.07016295939683914\n",
      "Epoch 52/60, Batch 95/100, Loss: 0.0613238662481308\n",
      "Epoch 52/60, Batch 96/100, Loss: 0.05547567829489708\n",
      "Epoch 52/60, Batch 97/100, Loss: 0.09428440034389496\n",
      "Epoch 52/60, Batch 98/100, Loss: 0.059405844658613205\n",
      "Epoch 52/60, Batch 99/100, Loss: 0.1359729915857315\n",
      "Epoch 52/60, Batch 100/100, Loss: 0.1303820163011551\n",
      "Saved model parameters to models/unet_epoch52.pdparams\n",
      "Epoch 53/60, Batch 1/100, Loss: 0.06230923533439636\n",
      "Epoch 53/60, Batch 2/100, Loss: 0.03523702174425125\n",
      "Epoch 53/60, Batch 3/100, Loss: 0.1369534134864807\n",
      "Epoch 53/60, Batch 4/100, Loss: 0.04933977127075195\n",
      "Epoch 53/60, Batch 5/100, Loss: 0.09730520099401474\n",
      "Epoch 53/60, Batch 6/100, Loss: 0.10241297632455826\n",
      "Epoch 53/60, Batch 7/100, Loss: 0.06955085694789886\n",
      "Epoch 53/60, Batch 8/100, Loss: 0.04439951106905937\n",
      "Epoch 53/60, Batch 9/100, Loss: 0.16554617881774902\n",
      "Epoch 53/60, Batch 10/100, Loss: 0.041394639760255814\n",
      "Epoch 53/60, Batch 11/100, Loss: 0.021685916930437088\n",
      "Epoch 53/60, Batch 12/100, Loss: 0.031778283417224884\n",
      "Epoch 53/60, Batch 13/100, Loss: 0.023295432329177856\n",
      "Epoch 53/60, Batch 14/100, Loss: 0.028681166470050812\n",
      "Epoch 53/60, Batch 15/100, Loss: 0.09321075677871704\n",
      "Epoch 53/60, Batch 16/100, Loss: 0.0526590570807457\n",
      "Epoch 53/60, Batch 17/100, Loss: 0.03353608772158623\n",
      "Epoch 53/60, Batch 18/100, Loss: 0.10517179220914841\n",
      "Epoch 53/60, Batch 19/100, Loss: 0.02470041997730732\n",
      "Epoch 53/60, Batch 20/100, Loss: 0.12847383320331573\n",
      "Epoch 53/60, Batch 21/100, Loss: 0.03271057829260826\n",
      "Epoch 53/60, Batch 22/100, Loss: 0.0508316271007061\n",
      "Epoch 53/60, Batch 23/100, Loss: 0.07913316041231155\n",
      "Epoch 53/60, Batch 24/100, Loss: 0.04190555959939957\n",
      "Epoch 53/60, Batch 25/100, Loss: 0.04864397644996643\n",
      "Epoch 53/60, Batch 26/100, Loss: 0.10487955808639526\n",
      "Epoch 53/60, Batch 27/100, Loss: 0.04962397366762161\n",
      "Epoch 53/60, Batch 28/100, Loss: 0.06223263218998909\n",
      "Epoch 53/60, Batch 29/100, Loss: 0.072479248046875\n",
      "Epoch 53/60, Batch 30/100, Loss: 0.04309746250510216\n",
      "Epoch 53/60, Batch 31/100, Loss: 0.07671748846769333\n",
      "Epoch 53/60, Batch 32/100, Loss: 0.030585993081331253\n",
      "Epoch 53/60, Batch 33/100, Loss: 0.02759064920246601\n",
      "Epoch 53/60, Batch 34/100, Loss: 0.0827518105506897\n",
      "Epoch 53/60, Batch 35/100, Loss: 0.07075408101081848\n",
      "Epoch 53/60, Batch 36/100, Loss: 0.13500258326530457\n",
      "Epoch 53/60, Batch 37/100, Loss: 0.07670921087265015\n",
      "Epoch 53/60, Batch 38/100, Loss: 0.038423649966716766\n",
      "Epoch 53/60, Batch 39/100, Loss: 0.04348620027303696\n",
      "Epoch 53/60, Batch 40/100, Loss: 0.05799286067485809\n",
      "Epoch 53/60, Batch 41/100, Loss: 0.0839506983757019\n",
      "Epoch 53/60, Batch 42/100, Loss: 0.07999199628829956\n",
      "Epoch 53/60, Batch 43/100, Loss: 0.034047387540340424\n",
      "Epoch 53/60, Batch 44/100, Loss: 0.033058129251003265\n",
      "Epoch 53/60, Batch 45/100, Loss: 0.054407987743616104\n",
      "Epoch 53/60, Batch 46/100, Loss: 0.28573599457740784\n",
      "Epoch 53/60, Batch 47/100, Loss: 0.07229296118021011\n",
      "Epoch 53/60, Batch 48/100, Loss: 0.04728617146611214\n",
      "Epoch 53/60, Batch 49/100, Loss: 0.045445434749126434\n",
      "Epoch 53/60, Batch 50/100, Loss: 0.10585825145244598\n",
      "Epoch 53/60, Batch 51/100, Loss: 0.045414213091135025\n",
      "Epoch 53/60, Batch 52/100, Loss: 0.0897895097732544\n",
      "Epoch 53/60, Batch 53/100, Loss: 0.07078403979539871\n",
      "Epoch 53/60, Batch 54/100, Loss: 0.07786495983600616\n",
      "Epoch 53/60, Batch 55/100, Loss: 0.07397891581058502\n",
      "Epoch 53/60, Batch 56/100, Loss: 0.05283763259649277\n",
      "Epoch 53/60, Batch 57/100, Loss: 0.14045412838459015\n",
      "Epoch 53/60, Batch 58/100, Loss: 0.029314273968338966\n",
      "Epoch 53/60, Batch 59/100, Loss: 0.06845296174287796\n",
      "Epoch 53/60, Batch 60/100, Loss: 0.07151134312152863\n",
      "Epoch 53/60, Batch 61/100, Loss: 0.11012637615203857\n",
      "Epoch 53/60, Batch 62/100, Loss: 0.05099014192819595\n",
      "Epoch 53/60, Batch 63/100, Loss: 0.050878558307886124\n",
      "Epoch 53/60, Batch 64/100, Loss: 0.0352344736456871\n",
      "Epoch 53/60, Batch 65/100, Loss: 0.0416480116546154\n",
      "Epoch 53/60, Batch 66/100, Loss: 0.08210164308547974\n",
      "Epoch 53/60, Batch 67/100, Loss: 0.0502152144908905\n",
      "Epoch 53/60, Batch 68/100, Loss: 0.07933545112609863\n",
      "Epoch 53/60, Batch 69/100, Loss: 0.028183281421661377\n",
      "Epoch 53/60, Batch 70/100, Loss: 0.06728199124336243\n",
      "Epoch 53/60, Batch 71/100, Loss: 0.057937949895858765\n",
      "Epoch 53/60, Batch 72/100, Loss: 0.07914933562278748\n",
      "Epoch 53/60, Batch 73/100, Loss: 0.054874107241630554\n",
      "Epoch 53/60, Batch 74/100, Loss: 0.06841902434825897\n",
      "Epoch 53/60, Batch 75/100, Loss: 0.04344286769628525\n",
      "Epoch 53/60, Batch 76/100, Loss: 0.02913040481507778\n",
      "Epoch 53/60, Batch 77/100, Loss: 0.017613712698221207\n",
      "Epoch 53/60, Batch 78/100, Loss: 0.022518128156661987\n",
      "Epoch 53/60, Batch 79/100, Loss: 0.024128949269652367\n",
      "Epoch 53/60, Batch 80/100, Loss: 0.03605986386537552\n",
      "Epoch 53/60, Batch 81/100, Loss: 0.038653481751680374\n",
      "Epoch 53/60, Batch 82/100, Loss: 0.05215742811560631\n",
      "Epoch 53/60, Batch 83/100, Loss: 0.022512316703796387\n",
      "Epoch 53/60, Batch 84/100, Loss: 0.04292872175574303\n",
      "Epoch 53/60, Batch 85/100, Loss: 0.07841470092535019\n",
      "Epoch 53/60, Batch 86/100, Loss: 0.03154780715703964\n",
      "Epoch 53/60, Batch 87/100, Loss: 0.03526366502046585\n",
      "Epoch 53/60, Batch 88/100, Loss: 0.07371657341718674\n",
      "Epoch 53/60, Batch 89/100, Loss: 0.0596097931265831\n",
      "Epoch 53/60, Batch 90/100, Loss: 0.055459536612033844\n",
      "Epoch 53/60, Batch 91/100, Loss: 0.04607526957988739\n",
      "Epoch 53/60, Batch 92/100, Loss: 0.03448480740189552\n",
      "Epoch 53/60, Batch 93/100, Loss: 0.07907210290431976\n",
      "Epoch 53/60, Batch 94/100, Loss: 0.03149287402629852\n",
      "Epoch 53/60, Batch 95/100, Loss: 0.05003286525607109\n",
      "Epoch 53/60, Batch 96/100, Loss: 0.049036167562007904\n",
      "Epoch 53/60, Batch 97/100, Loss: 0.07275800406932831\n",
      "Epoch 53/60, Batch 98/100, Loss: 0.1743042767047882\n",
      "Epoch 53/60, Batch 99/100, Loss: 0.07344784587621689\n",
      "Epoch 53/60, Batch 100/100, Loss: 0.05981351435184479\n",
      "Saved model parameters to models/unet_epoch53.pdparams\n",
      "Epoch 54/60, Batch 1/100, Loss: 0.13449104130268097\n",
      "Epoch 54/60, Batch 2/100, Loss: 0.021033456549048424\n",
      "Epoch 54/60, Batch 3/100, Loss: 0.08032521605491638\n",
      "Epoch 54/60, Batch 4/100, Loss: 0.11054878681898117\n",
      "Epoch 54/60, Batch 5/100, Loss: 0.0490623340010643\n",
      "Epoch 54/60, Batch 6/100, Loss: 0.06110420450568199\n",
      "Epoch 54/60, Batch 7/100, Loss: 0.03886949643492699\n",
      "Epoch 54/60, Batch 8/100, Loss: 0.0548657551407814\n",
      "Epoch 54/60, Batch 9/100, Loss: 0.07118107378482819\n",
      "Epoch 54/60, Batch 10/100, Loss: 0.13143788278102875\n",
      "Epoch 54/60, Batch 11/100, Loss: 0.08710385113954544\n",
      "Epoch 54/60, Batch 12/100, Loss: 0.024557000026106834\n",
      "Epoch 54/60, Batch 13/100, Loss: 0.07356821000576019\n",
      "Epoch 54/60, Batch 14/100, Loss: 0.06272836029529572\n",
      "Epoch 54/60, Batch 15/100, Loss: 0.0853695422410965\n",
      "Epoch 54/60, Batch 16/100, Loss: 0.04123436287045479\n",
      "Epoch 54/60, Batch 17/100, Loss: 0.1156419962644577\n",
      "Epoch 54/60, Batch 18/100, Loss: 0.056134626269340515\n",
      "Epoch 54/60, Batch 19/100, Loss: 0.04959110915660858\n",
      "Epoch 54/60, Batch 20/100, Loss: 0.046796564012765884\n",
      "Epoch 54/60, Batch 21/100, Loss: 0.0394512340426445\n",
      "Epoch 54/60, Batch 22/100, Loss: 0.04094719886779785\n",
      "Epoch 54/60, Batch 23/100, Loss: 0.017732294276356697\n",
      "Epoch 54/60, Batch 24/100, Loss: 0.046069636940956116\n",
      "Epoch 54/60, Batch 25/100, Loss: 0.09609083086252213\n",
      "Epoch 54/60, Batch 26/100, Loss: 0.05143699049949646\n",
      "Epoch 54/60, Batch 27/100, Loss: 0.02481831982731819\n",
      "Epoch 54/60, Batch 28/100, Loss: 0.04596259072422981\n",
      "Epoch 54/60, Batch 29/100, Loss: 0.04711947962641716\n",
      "Epoch 54/60, Batch 30/100, Loss: 0.14447236061096191\n",
      "Epoch 54/60, Batch 31/100, Loss: 0.026882220059633255\n",
      "Epoch 54/60, Batch 32/100, Loss: 0.048557646572589874\n",
      "Epoch 54/60, Batch 33/100, Loss: 0.019013255834579468\n",
      "Epoch 54/60, Batch 34/100, Loss: 0.06556542217731476\n",
      "Epoch 54/60, Batch 35/100, Loss: 0.06369167566299438\n",
      "Epoch 54/60, Batch 36/100, Loss: 0.038554947823286057\n",
      "Epoch 54/60, Batch 37/100, Loss: 0.03340919688344002\n",
      "Epoch 54/60, Batch 38/100, Loss: 0.06643541157245636\n",
      "Epoch 54/60, Batch 39/100, Loss: 0.059784889221191406\n",
      "Epoch 54/60, Batch 40/100, Loss: 0.06600958108901978\n",
      "Epoch 54/60, Batch 41/100, Loss: 0.07383346557617188\n",
      "Epoch 54/60, Batch 42/100, Loss: 0.030248792842030525\n",
      "Epoch 54/60, Batch 43/100, Loss: 0.02415907010436058\n",
      "Epoch 54/60, Batch 44/100, Loss: 0.03611839935183525\n",
      "Epoch 54/60, Batch 45/100, Loss: 0.06739311665296555\n",
      "Epoch 54/60, Batch 46/100, Loss: 0.07916226238012314\n",
      "Epoch 54/60, Batch 47/100, Loss: 0.0823841392993927\n",
      "Epoch 54/60, Batch 48/100, Loss: 0.05359542742371559\n",
      "Epoch 54/60, Batch 49/100, Loss: 0.07501472532749176\n",
      "Epoch 54/60, Batch 50/100, Loss: 0.06937829405069351\n",
      "Epoch 54/60, Batch 51/100, Loss: 0.07121827453374863\n",
      "Epoch 54/60, Batch 52/100, Loss: 0.05009409412741661\n",
      "Epoch 54/60, Batch 53/100, Loss: 0.029314089566469193\n",
      "Epoch 54/60, Batch 54/100, Loss: 0.03880518302321434\n",
      "Epoch 54/60, Batch 55/100, Loss: 0.06189500540494919\n",
      "Epoch 54/60, Batch 56/100, Loss: 0.10008600354194641\n",
      "Epoch 54/60, Batch 57/100, Loss: 0.06994903087615967\n",
      "Epoch 54/60, Batch 58/100, Loss: 0.01998957060277462\n",
      "Epoch 54/60, Batch 59/100, Loss: 0.030443614348769188\n",
      "Epoch 54/60, Batch 60/100, Loss: 0.06011979281902313\n",
      "Epoch 54/60, Batch 61/100, Loss: 0.01681622862815857\n",
      "Epoch 54/60, Batch 62/100, Loss: 0.038454774767160416\n",
      "Epoch 54/60, Batch 63/100, Loss: 0.08030367642641068\n",
      "Epoch 54/60, Batch 64/100, Loss: 0.022451885044574738\n",
      "Epoch 54/60, Batch 65/100, Loss: 0.1362863928079605\n",
      "Epoch 54/60, Batch 66/100, Loss: 0.034585773944854736\n",
      "Epoch 54/60, Batch 67/100, Loss: 0.02772071212530136\n",
      "Epoch 54/60, Batch 68/100, Loss: 0.052783917635679245\n",
      "Epoch 54/60, Batch 69/100, Loss: 0.0266242865473032\n",
      "Epoch 54/60, Batch 70/100, Loss: 0.056294143199920654\n",
      "Epoch 54/60, Batch 71/100, Loss: 0.060644399374723434\n",
      "Epoch 54/60, Batch 72/100, Loss: 0.11546420305967331\n",
      "Epoch 54/60, Batch 73/100, Loss: 0.05778154358267784\n",
      "Epoch 54/60, Batch 74/100, Loss: 0.1373887062072754\n",
      "Epoch 54/60, Batch 75/100, Loss: 0.14546416699886322\n",
      "Epoch 54/60, Batch 76/100, Loss: 0.06581294536590576\n",
      "Epoch 54/60, Batch 77/100, Loss: 0.053470052778720856\n",
      "Epoch 54/60, Batch 78/100, Loss: 0.03672606870532036\n",
      "Epoch 54/60, Batch 79/100, Loss: 0.10888099670410156\n",
      "Epoch 54/60, Batch 80/100, Loss: 0.027750061824917793\n",
      "Epoch 54/60, Batch 81/100, Loss: 0.026088358834385872\n",
      "Epoch 54/60, Batch 82/100, Loss: 0.1676095724105835\n",
      "Epoch 54/60, Batch 83/100, Loss: 0.0502961203455925\n",
      "Epoch 54/60, Batch 84/100, Loss: 0.0636669397354126\n",
      "Epoch 54/60, Batch 85/100, Loss: 0.07817907631397247\n",
      "Epoch 54/60, Batch 86/100, Loss: 0.0712246522307396\n",
      "Epoch 54/60, Batch 87/100, Loss: 0.09218636155128479\n",
      "Epoch 54/60, Batch 88/100, Loss: 0.028561610728502274\n",
      "Epoch 54/60, Batch 89/100, Loss: 0.17621465027332306\n",
      "Epoch 54/60, Batch 90/100, Loss: 0.04546040669083595\n",
      "Epoch 54/60, Batch 91/100, Loss: 0.04147989675402641\n",
      "Epoch 54/60, Batch 92/100, Loss: 0.07424480468034744\n",
      "Epoch 54/60, Batch 93/100, Loss: 0.06600022315979004\n",
      "Epoch 54/60, Batch 94/100, Loss: 0.0816880464553833\n",
      "Epoch 54/60, Batch 95/100, Loss: 0.06487937271595001\n",
      "Epoch 54/60, Batch 96/100, Loss: 0.048672035336494446\n",
      "Epoch 54/60, Batch 97/100, Loss: 0.05088483542203903\n",
      "Epoch 54/60, Batch 98/100, Loss: 0.040181223303079605\n",
      "Epoch 54/60, Batch 99/100, Loss: 0.14856362342834473\n",
      "Epoch 54/60, Batch 100/100, Loss: 0.06616663187742233\n",
      "Saved model parameters to models/unet_epoch54.pdparams\n",
      "Epoch 55/60, Batch 1/100, Loss: 0.05251196026802063\n",
      "Epoch 55/60, Batch 2/100, Loss: 0.09480930119752884\n",
      "Epoch 55/60, Batch 3/100, Loss: 0.1864572912454605\n",
      "Epoch 55/60, Batch 4/100, Loss: 0.08631207048892975\n",
      "Epoch 55/60, Batch 5/100, Loss: 0.09881579875946045\n",
      "Epoch 55/60, Batch 6/100, Loss: 0.05042443051934242\n",
      "Epoch 55/60, Batch 7/100, Loss: 0.04201647639274597\n",
      "Epoch 55/60, Batch 8/100, Loss: 0.05374525859951973\n",
      "Epoch 55/60, Batch 9/100, Loss: 0.0545976459980011\n",
      "Epoch 55/60, Batch 10/100, Loss: 0.05246319994330406\n",
      "Epoch 55/60, Batch 11/100, Loss: 0.07200200855731964\n",
      "Epoch 55/60, Batch 12/100, Loss: 0.05744810029864311\n",
      "Epoch 55/60, Batch 13/100, Loss: 0.09817303717136383\n",
      "Epoch 55/60, Batch 14/100, Loss: 0.0365382619202137\n",
      "Epoch 55/60, Batch 15/100, Loss: 0.022901328280568123\n",
      "Epoch 55/60, Batch 16/100, Loss: 0.01685754954814911\n",
      "Epoch 55/60, Batch 17/100, Loss: 0.09454406052827835\n",
      "Epoch 55/60, Batch 18/100, Loss: 0.08580885082483292\n",
      "Epoch 55/60, Batch 19/100, Loss: 0.08563972264528275\n",
      "Epoch 55/60, Batch 20/100, Loss: 0.0542932003736496\n",
      "Epoch 55/60, Batch 21/100, Loss: 0.04535028710961342\n",
      "Epoch 55/60, Batch 22/100, Loss: 0.07171110808849335\n",
      "Epoch 55/60, Batch 23/100, Loss: 0.11698990315198898\n",
      "Epoch 55/60, Batch 24/100, Loss: 0.18179641664028168\n",
      "Epoch 55/60, Batch 25/100, Loss: 0.063715860247612\n",
      "Epoch 55/60, Batch 26/100, Loss: 0.07882867753505707\n",
      "Epoch 55/60, Batch 27/100, Loss: 0.1144014447927475\n",
      "Epoch 55/60, Batch 28/100, Loss: 0.05741739645600319\n",
      "Epoch 55/60, Batch 29/100, Loss: 0.12080971151590347\n",
      "Epoch 55/60, Batch 30/100, Loss: 0.11791595816612244\n",
      "Epoch 55/60, Batch 31/100, Loss: 0.08255273103713989\n",
      "Epoch 55/60, Batch 32/100, Loss: 0.053486429154872894\n",
      "Epoch 55/60, Batch 33/100, Loss: 0.11724507063627243\n",
      "Epoch 55/60, Batch 34/100, Loss: 0.02626216784119606\n",
      "Epoch 55/60, Batch 35/100, Loss: 0.04993318021297455\n",
      "Epoch 55/60, Batch 36/100, Loss: 0.10314442962408066\n",
      "Epoch 55/60, Batch 37/100, Loss: 0.02963419444859028\n",
      "Epoch 55/60, Batch 38/100, Loss: 0.0651494488120079\n",
      "Epoch 55/60, Batch 39/100, Loss: 0.11766766756772995\n",
      "Epoch 55/60, Batch 40/100, Loss: 0.06876710057258606\n",
      "Epoch 55/60, Batch 41/100, Loss: 0.05394599214196205\n",
      "Epoch 55/60, Batch 42/100, Loss: 0.02082446962594986\n",
      "Epoch 55/60, Batch 43/100, Loss: 0.06442412734031677\n",
      "Epoch 55/60, Batch 44/100, Loss: 0.03997565433382988\n",
      "Epoch 55/60, Batch 45/100, Loss: 0.05330013483762741\n",
      "Epoch 55/60, Batch 46/100, Loss: 0.051715187728405\n",
      "Epoch 55/60, Batch 47/100, Loss: 0.12796100974082947\n",
      "Epoch 55/60, Batch 48/100, Loss: 0.021052153781056404\n",
      "Epoch 55/60, Batch 49/100, Loss: 0.05069075524806976\n",
      "Epoch 55/60, Batch 50/100, Loss: 0.04148745909333229\n",
      "Epoch 55/60, Batch 51/100, Loss: 0.02554798126220703\n",
      "Epoch 55/60, Batch 52/100, Loss: 0.08182831108570099\n",
      "Epoch 55/60, Batch 53/100, Loss: 0.025884216651320457\n",
      "Epoch 55/60, Batch 54/100, Loss: 0.033396922051906586\n",
      "Epoch 55/60, Batch 55/100, Loss: 0.01940690539777279\n",
      "Epoch 55/60, Batch 56/100, Loss: 0.04999754950404167\n",
      "Epoch 55/60, Batch 57/100, Loss: 0.058858875185251236\n",
      "Epoch 55/60, Batch 58/100, Loss: 0.0943111926317215\n",
      "Epoch 55/60, Batch 59/100, Loss: 0.1730937957763672\n",
      "Epoch 55/60, Batch 60/100, Loss: 0.10729795694351196\n",
      "Epoch 55/60, Batch 61/100, Loss: 0.03933987393975258\n",
      "Epoch 55/60, Batch 62/100, Loss: 0.09734324365854263\n",
      "Epoch 55/60, Batch 63/100, Loss: 0.047238051891326904\n",
      "Epoch 55/60, Batch 64/100, Loss: 0.1022634208202362\n",
      "Epoch 55/60, Batch 65/100, Loss: 0.05735807865858078\n",
      "Epoch 55/60, Batch 66/100, Loss: 0.1676083505153656\n",
      "Epoch 55/60, Batch 67/100, Loss: 0.07328245788812637\n",
      "Epoch 55/60, Batch 68/100, Loss: 0.09114377945661545\n",
      "Epoch 55/60, Batch 69/100, Loss: 0.03889387473464012\n",
      "Epoch 55/60, Batch 70/100, Loss: 0.05582769587635994\n",
      "Epoch 55/60, Batch 71/100, Loss: 0.09747476130723953\n",
      "Epoch 55/60, Batch 72/100, Loss: 0.08271949738264084\n",
      "Epoch 55/60, Batch 73/100, Loss: 0.06738396733999252\n",
      "Epoch 55/60, Batch 74/100, Loss: 0.05109337717294693\n",
      "Epoch 55/60, Batch 75/100, Loss: 0.028304483741521835\n",
      "Epoch 55/60, Batch 76/100, Loss: 0.05047973245382309\n",
      "Epoch 55/60, Batch 77/100, Loss: 0.057553041726350784\n",
      "Epoch 55/60, Batch 78/100, Loss: 0.04554536193609238\n",
      "Epoch 55/60, Batch 79/100, Loss: 0.04604191705584526\n",
      "Epoch 55/60, Batch 80/100, Loss: 0.021811259910464287\n",
      "Epoch 55/60, Batch 81/100, Loss: 0.025140851736068726\n",
      "Epoch 55/60, Batch 82/100, Loss: 0.07802446186542511\n",
      "Epoch 55/60, Batch 83/100, Loss: 0.03710166737437248\n",
      "Epoch 55/60, Batch 84/100, Loss: 0.03062180057168007\n",
      "Epoch 55/60, Batch 85/100, Loss: 0.018632017076015472\n",
      "Epoch 55/60, Batch 86/100, Loss: 0.0317869633436203\n",
      "Epoch 55/60, Batch 87/100, Loss: 0.049997009336948395\n",
      "Epoch 55/60, Batch 88/100, Loss: 0.033078424632549286\n",
      "Epoch 55/60, Batch 89/100, Loss: 0.007948070764541626\n",
      "Epoch 55/60, Batch 90/100, Loss: 0.022971436381340027\n",
      "Epoch 55/60, Batch 91/100, Loss: 0.1023520678281784\n",
      "Epoch 55/60, Batch 92/100, Loss: 0.1332676112651825\n",
      "Epoch 55/60, Batch 93/100, Loss: 0.0932973250746727\n",
      "Epoch 55/60, Batch 94/100, Loss: 0.04146486520767212\n",
      "Epoch 55/60, Batch 95/100, Loss: 0.03982466831803322\n",
      "Epoch 55/60, Batch 96/100, Loss: 0.0214379895478487\n",
      "Epoch 55/60, Batch 97/100, Loss: 0.038831185549497604\n",
      "Epoch 55/60, Batch 98/100, Loss: 0.10588987171649933\n",
      "Epoch 55/60, Batch 99/100, Loss: 0.06206868961453438\n",
      "Epoch 55/60, Batch 100/100, Loss: 0.04844171553850174\n",
      "Saved model parameters to models/unet_epoch55.pdparams\n",
      "Epoch 56/60, Batch 1/100, Loss: 0.03260383382439613\n",
      "Epoch 56/60, Batch 2/100, Loss: 0.049942657351493835\n",
      "Epoch 56/60, Batch 3/100, Loss: 0.07593388110399246\n",
      "Epoch 56/60, Batch 4/100, Loss: 0.04621727019548416\n",
      "Epoch 56/60, Batch 5/100, Loss: 0.05149172991514206\n",
      "Epoch 56/60, Batch 6/100, Loss: 0.05601723864674568\n",
      "Epoch 56/60, Batch 7/100, Loss: 0.05194762349128723\n",
      "Epoch 56/60, Batch 8/100, Loss: 0.06233695521950722\n",
      "Epoch 56/60, Batch 9/100, Loss: 0.044080089777708054\n",
      "Epoch 56/60, Batch 10/100, Loss: 0.04713376238942146\n",
      "Epoch 56/60, Batch 11/100, Loss: 0.05586688593029976\n",
      "Epoch 56/60, Batch 12/100, Loss: 0.06323901563882828\n",
      "Epoch 56/60, Batch 13/100, Loss: 0.02825060859322548\n",
      "Epoch 56/60, Batch 14/100, Loss: 0.027462150901556015\n",
      "Epoch 56/60, Batch 15/100, Loss: 0.11272649466991425\n",
      "Epoch 56/60, Batch 16/100, Loss: 0.04006972908973694\n",
      "Epoch 56/60, Batch 17/100, Loss: 0.041202008724212646\n",
      "Epoch 56/60, Batch 18/100, Loss: 0.03924894332885742\n",
      "Epoch 56/60, Batch 19/100, Loss: 0.04855739325284958\n",
      "Epoch 56/60, Batch 20/100, Loss: 0.07121466100215912\n",
      "Epoch 56/60, Batch 21/100, Loss: 0.0986892580986023\n",
      "Epoch 56/60, Batch 22/100, Loss: 0.033033356070518494\n",
      "Epoch 56/60, Batch 23/100, Loss: 0.04040994122624397\n",
      "Epoch 56/60, Batch 24/100, Loss: 0.050842151045799255\n",
      "Epoch 56/60, Batch 25/100, Loss: 0.02151106484234333\n",
      "Epoch 56/60, Batch 26/100, Loss: 0.10007274895906448\n",
      "Epoch 56/60, Batch 27/100, Loss: 0.08730411529541016\n",
      "Epoch 56/60, Batch 28/100, Loss: 0.09664064645767212\n",
      "Epoch 56/60, Batch 29/100, Loss: 0.055481597781181335\n",
      "Epoch 56/60, Batch 30/100, Loss: 0.06767872720956802\n",
      "Epoch 56/60, Batch 31/100, Loss: 0.10519344359636307\n",
      "Epoch 56/60, Batch 32/100, Loss: 0.13968618214130402\n",
      "Epoch 56/60, Batch 33/100, Loss: 0.05448300391435623\n",
      "Epoch 56/60, Batch 34/100, Loss: 0.04328432306647301\n",
      "Epoch 56/60, Batch 35/100, Loss: 0.06909269094467163\n",
      "Epoch 56/60, Batch 36/100, Loss: 0.10893252491950989\n",
      "Epoch 56/60, Batch 37/100, Loss: 0.04958951473236084\n",
      "Epoch 56/60, Batch 38/100, Loss: 0.07459686696529388\n",
      "Epoch 56/60, Batch 39/100, Loss: 0.02826019376516342\n",
      "Epoch 56/60, Batch 40/100, Loss: 0.15137240290641785\n",
      "Epoch 56/60, Batch 41/100, Loss: 0.04443402960896492\n",
      "Epoch 56/60, Batch 42/100, Loss: 0.06288176029920578\n",
      "Epoch 56/60, Batch 43/100, Loss: 0.05370672047138214\n",
      "Epoch 56/60, Batch 44/100, Loss: 0.07507475465536118\n",
      "Epoch 56/60, Batch 45/100, Loss: 0.040883541107177734\n",
      "Epoch 56/60, Batch 46/100, Loss: 0.10085989534854889\n",
      "Epoch 56/60, Batch 47/100, Loss: 0.03771466016769409\n",
      "Epoch 56/60, Batch 48/100, Loss: 0.0652756616473198\n",
      "Epoch 56/60, Batch 49/100, Loss: 0.018870463594794273\n",
      "Epoch 56/60, Batch 50/100, Loss: 0.02991108037531376\n",
      "Epoch 56/60, Batch 51/100, Loss: 0.09314422309398651\n",
      "Epoch 56/60, Batch 52/100, Loss: 0.08911920338869095\n",
      "Epoch 56/60, Batch 53/100, Loss: 0.03247174620628357\n",
      "Epoch 56/60, Batch 54/100, Loss: 0.08731850981712341\n",
      "Epoch 56/60, Batch 55/100, Loss: 0.026744719594717026\n",
      "Epoch 56/60, Batch 56/100, Loss: 0.14065751433372498\n",
      "Epoch 56/60, Batch 57/100, Loss: 0.04161130636930466\n",
      "Epoch 56/60, Batch 58/100, Loss: 0.023843832314014435\n",
      "Epoch 56/60, Batch 59/100, Loss: 0.0476473793387413\n",
      "Epoch 56/60, Batch 60/100, Loss: 0.08257320523262024\n",
      "Epoch 56/60, Batch 61/100, Loss: 0.0462476871907711\n",
      "Epoch 56/60, Batch 62/100, Loss: 0.10169863700866699\n",
      "Epoch 56/60, Batch 63/100, Loss: 0.03941132873296738\n",
      "Epoch 56/60, Batch 64/100, Loss: 0.09311819821596146\n",
      "Epoch 56/60, Batch 65/100, Loss: 0.026792190968990326\n",
      "Epoch 56/60, Batch 66/100, Loss: 0.06074913963675499\n",
      "Epoch 56/60, Batch 67/100, Loss: 0.0651167556643486\n",
      "Epoch 56/60, Batch 68/100, Loss: 0.0564100481569767\n",
      "Epoch 56/60, Batch 69/100, Loss: 0.11442740261554718\n",
      "Epoch 56/60, Batch 70/100, Loss: 0.08835405111312866\n",
      "Epoch 56/60, Batch 71/100, Loss: 0.02231914922595024\n",
      "Epoch 56/60, Batch 72/100, Loss: 0.14194469153881073\n",
      "Epoch 56/60, Batch 73/100, Loss: 0.05483471602201462\n",
      "Epoch 56/60, Batch 74/100, Loss: 0.07623979449272156\n",
      "Epoch 56/60, Batch 75/100, Loss: 0.033117834478616714\n",
      "Epoch 56/60, Batch 76/100, Loss: 0.055821724236011505\n",
      "Epoch 56/60, Batch 77/100, Loss: 0.059104274958372116\n",
      "Epoch 56/60, Batch 78/100, Loss: 0.027110690250992775\n",
      "Epoch 56/60, Batch 79/100, Loss: 0.02674754336476326\n",
      "Epoch 56/60, Batch 80/100, Loss: 0.15912215411663055\n",
      "Epoch 56/60, Batch 81/100, Loss: 0.09890750050544739\n",
      "Epoch 56/60, Batch 82/100, Loss: 0.01883472315967083\n",
      "Epoch 56/60, Batch 83/100, Loss: 0.07324535399675369\n",
      "Epoch 56/60, Batch 84/100, Loss: 0.053220994770526886\n",
      "Epoch 56/60, Batch 85/100, Loss: 0.06464201211929321\n",
      "Epoch 56/60, Batch 86/100, Loss: 0.0637005940079689\n",
      "Epoch 56/60, Batch 87/100, Loss: 0.059485502541065216\n",
      "Epoch 56/60, Batch 88/100, Loss: 0.05094745755195618\n",
      "Epoch 56/60, Batch 89/100, Loss: 0.02983766980469227\n",
      "Epoch 56/60, Batch 90/100, Loss: 0.03984326496720314\n",
      "Epoch 56/60, Batch 91/100, Loss: 0.04955515265464783\n",
      "Epoch 56/60, Batch 92/100, Loss: 0.0337475910782814\n",
      "Epoch 56/60, Batch 93/100, Loss: 0.03785434737801552\n",
      "Epoch 56/60, Batch 94/100, Loss: 0.030408790335059166\n",
      "Epoch 56/60, Batch 95/100, Loss: 0.0685780942440033\n",
      "Epoch 56/60, Batch 96/100, Loss: 0.05210433155298233\n",
      "Epoch 56/60, Batch 97/100, Loss: 0.030498553067445755\n",
      "Epoch 56/60, Batch 98/100, Loss: 0.031307339668273926\n",
      "Epoch 56/60, Batch 99/100, Loss: 0.059426870197057724\n",
      "Epoch 56/60, Batch 100/100, Loss: 0.13273783028125763\n",
      "Saved model parameters to models/unet_epoch56.pdparams\n",
      "Epoch 57/60, Batch 1/100, Loss: 0.23647430539131165\n",
      "Epoch 57/60, Batch 2/100, Loss: 0.017845969647169113\n",
      "Epoch 57/60, Batch 3/100, Loss: 0.05730587616562843\n",
      "Epoch 57/60, Batch 4/100, Loss: 0.05330170318484306\n",
      "Epoch 57/60, Batch 5/100, Loss: 0.024015752598643303\n",
      "Epoch 57/60, Batch 6/100, Loss: 0.05973177030682564\n",
      "Epoch 57/60, Batch 7/100, Loss: 0.017501339316368103\n",
      "Epoch 57/60, Batch 8/100, Loss: 0.10661506652832031\n",
      "Epoch 57/60, Batch 9/100, Loss: 0.052951861172914505\n",
      "Epoch 57/60, Batch 10/100, Loss: 0.09399108588695526\n",
      "Epoch 57/60, Batch 11/100, Loss: 0.09611813724040985\n",
      "Epoch 57/60, Batch 12/100, Loss: 0.02947770059108734\n",
      "Epoch 57/60, Batch 13/100, Loss: 0.03451651707291603\n",
      "Epoch 57/60, Batch 14/100, Loss: 0.0591033473610878\n",
      "Epoch 57/60, Batch 15/100, Loss: 0.03693951293826103\n",
      "Epoch 57/60, Batch 16/100, Loss: 0.03684620559215546\n",
      "Epoch 57/60, Batch 17/100, Loss: 0.023438606411218643\n",
      "Epoch 57/60, Batch 18/100, Loss: 0.057415466755628586\n",
      "Epoch 57/60, Batch 19/100, Loss: 0.080192431807518\n",
      "Epoch 57/60, Batch 20/100, Loss: 0.04112562909722328\n",
      "Epoch 57/60, Batch 21/100, Loss: 0.04399711266160011\n",
      "Epoch 57/60, Batch 22/100, Loss: 0.06278049200773239\n",
      "Epoch 57/60, Batch 23/100, Loss: 0.07517431676387787\n",
      "Epoch 57/60, Batch 24/100, Loss: 0.04012438654899597\n",
      "Epoch 57/60, Batch 25/100, Loss: 0.03029807098209858\n",
      "Epoch 57/60, Batch 26/100, Loss: 0.031732287257909775\n",
      "Epoch 57/60, Batch 27/100, Loss: 0.023216094821691513\n",
      "Epoch 57/60, Batch 28/100, Loss: 0.01634581759572029\n",
      "Epoch 57/60, Batch 29/100, Loss: 0.04939454048871994\n",
      "Epoch 57/60, Batch 30/100, Loss: 0.025082603096961975\n",
      "Epoch 57/60, Batch 31/100, Loss: 0.1223125085234642\n",
      "Epoch 57/60, Batch 32/100, Loss: 0.14109420776367188\n",
      "Epoch 57/60, Batch 33/100, Loss: 0.0868188738822937\n",
      "Epoch 57/60, Batch 34/100, Loss: 0.03216507285833359\n",
      "Epoch 57/60, Batch 35/100, Loss: 0.07063838094472885\n",
      "Epoch 57/60, Batch 36/100, Loss: 0.03519047424197197\n",
      "Epoch 57/60, Batch 37/100, Loss: 0.0760096088051796\n",
      "Epoch 57/60, Batch 38/100, Loss: 0.04422416538000107\n",
      "Epoch 57/60, Batch 39/100, Loss: 0.02211395837366581\n",
      "Epoch 57/60, Batch 40/100, Loss: 0.10432451963424683\n",
      "Epoch 57/60, Batch 41/100, Loss: 0.06084135174751282\n",
      "Epoch 57/60, Batch 42/100, Loss: 0.051682066172361374\n",
      "Epoch 57/60, Batch 43/100, Loss: 0.08881791681051254\n",
      "Epoch 57/60, Batch 44/100, Loss: 0.057122766971588135\n",
      "Epoch 57/60, Batch 45/100, Loss: 0.0397985503077507\n",
      "Epoch 57/60, Batch 46/100, Loss: 0.049709517508745193\n",
      "Epoch 57/60, Batch 47/100, Loss: 0.028259361162781715\n",
      "Epoch 57/60, Batch 48/100, Loss: 0.04993373900651932\n",
      "Epoch 57/60, Batch 49/100, Loss: 0.02989186719059944\n",
      "Epoch 57/60, Batch 50/100, Loss: 0.03459903970360756\n",
      "Epoch 57/60, Batch 51/100, Loss: 0.04877815395593643\n",
      "Epoch 57/60, Batch 52/100, Loss: 0.27633628249168396\n",
      "Epoch 57/60, Batch 53/100, Loss: 0.08838936686515808\n",
      "Epoch 57/60, Batch 54/100, Loss: 0.04918386787176132\n",
      "Epoch 57/60, Batch 55/100, Loss: 0.10723549872636795\n",
      "Epoch 57/60, Batch 56/100, Loss: 0.052910421043634415\n",
      "Epoch 57/60, Batch 57/100, Loss: 0.03133682906627655\n",
      "Epoch 57/60, Batch 58/100, Loss: 0.07079410552978516\n",
      "Epoch 57/60, Batch 59/100, Loss: 0.02919166535139084\n",
      "Epoch 57/60, Batch 60/100, Loss: 0.07564994692802429\n",
      "Epoch 57/60, Batch 61/100, Loss: 0.03888223320245743\n",
      "Epoch 57/60, Batch 62/100, Loss: 0.05453663691878319\n",
      "Epoch 57/60, Batch 63/100, Loss: 0.05620472878217697\n",
      "Epoch 57/60, Batch 64/100, Loss: 0.08595853298902512\n",
      "Epoch 57/60, Batch 65/100, Loss: 0.08081138134002686\n",
      "Epoch 57/60, Batch 66/100, Loss: 0.06870459020137787\n",
      "Epoch 57/60, Batch 67/100, Loss: 0.07063323259353638\n",
      "Epoch 57/60, Batch 68/100, Loss: 0.04667016118764877\n",
      "Epoch 57/60, Batch 69/100, Loss: 0.06268634647130966\n",
      "Epoch 57/60, Batch 70/100, Loss: 0.07700443267822266\n",
      "Epoch 57/60, Batch 71/100, Loss: 0.1034117341041565\n",
      "Epoch 57/60, Batch 72/100, Loss: 0.08188459277153015\n",
      "Epoch 57/60, Batch 73/100, Loss: 0.042112283408641815\n",
      "Epoch 57/60, Batch 74/100, Loss: 0.013671391643583775\n",
      "Epoch 57/60, Batch 75/100, Loss: 0.06187187135219574\n",
      "Epoch 57/60, Batch 76/100, Loss: 0.06597276777029037\n",
      "Epoch 57/60, Batch 77/100, Loss: 0.09544266015291214\n",
      "Epoch 57/60, Batch 78/100, Loss: 0.05783947929739952\n",
      "Epoch 57/60, Batch 79/100, Loss: 0.04178427532315254\n",
      "Epoch 57/60, Batch 80/100, Loss: 0.02230413258075714\n",
      "Epoch 57/60, Batch 81/100, Loss: 0.05304016172885895\n",
      "Epoch 57/60, Batch 82/100, Loss: 0.030902817845344543\n",
      "Epoch 57/60, Batch 83/100, Loss: 0.05474454164505005\n",
      "Epoch 57/60, Batch 84/100, Loss: 0.019777929410338402\n",
      "Epoch 57/60, Batch 85/100, Loss: 0.07749887555837631\n",
      "Epoch 57/60, Batch 86/100, Loss: 0.04339603707194328\n",
      "Epoch 57/60, Batch 87/100, Loss: 0.030819296836853027\n",
      "Epoch 57/60, Batch 88/100, Loss: 0.02419157326221466\n",
      "Epoch 57/60, Batch 89/100, Loss: 0.09865017235279083\n",
      "Epoch 57/60, Batch 90/100, Loss: 0.07690834999084473\n",
      "Epoch 57/60, Batch 91/100, Loss: 0.0530417338013649\n",
      "Epoch 57/60, Batch 92/100, Loss: 0.08717522025108337\n",
      "Epoch 57/60, Batch 93/100, Loss: 0.0719929039478302\n",
      "Epoch 57/60, Batch 94/100, Loss: 0.09929099678993225\n",
      "Epoch 57/60, Batch 95/100, Loss: 0.041991908103227615\n",
      "Epoch 57/60, Batch 96/100, Loss: 0.03138236328959465\n",
      "Epoch 57/60, Batch 97/100, Loss: 0.06625775992870331\n",
      "Epoch 57/60, Batch 98/100, Loss: 0.059849053621292114\n",
      "Epoch 57/60, Batch 99/100, Loss: 0.04334094375371933\n",
      "Epoch 57/60, Batch 100/100, Loss: 0.06318046152591705\n",
      "Saved model parameters to models/unet_epoch57.pdparams\n",
      "Epoch 58/60, Batch 1/100, Loss: 0.0463700145483017\n",
      "Epoch 58/60, Batch 2/100, Loss: 0.07350152730941772\n",
      "Epoch 58/60, Batch 3/100, Loss: 0.022667428478598595\n",
      "Epoch 58/60, Batch 4/100, Loss: 0.07711228728294373\n",
      "Epoch 58/60, Batch 5/100, Loss: 0.03475787118077278\n",
      "Epoch 58/60, Batch 6/100, Loss: 0.07622721046209335\n",
      "Epoch 58/60, Batch 7/100, Loss: 0.01956958882510662\n",
      "Epoch 58/60, Batch 8/100, Loss: 0.04512261599302292\n",
      "Epoch 58/60, Batch 9/100, Loss: 0.054219283163547516\n",
      "Epoch 58/60, Batch 10/100, Loss: 0.10245577991008759\n",
      "Epoch 58/60, Batch 11/100, Loss: 0.05481185391545296\n",
      "Epoch 58/60, Batch 12/100, Loss: 0.040294352918863297\n",
      "Epoch 58/60, Batch 13/100, Loss: 0.05408008396625519\n",
      "Epoch 58/60, Batch 14/100, Loss: 0.03507940098643303\n",
      "Epoch 58/60, Batch 15/100, Loss: 0.052759237587451935\n",
      "Epoch 58/60, Batch 16/100, Loss: 0.04318114370107651\n",
      "Epoch 58/60, Batch 17/100, Loss: 0.05741696432232857\n",
      "Epoch 58/60, Batch 18/100, Loss: 0.06162995845079422\n",
      "Epoch 58/60, Batch 19/100, Loss: 0.027766525745391846\n",
      "Epoch 58/60, Batch 20/100, Loss: 0.032236482948064804\n",
      "Epoch 58/60, Batch 21/100, Loss: 0.07417459785938263\n",
      "Epoch 58/60, Batch 22/100, Loss: 0.07621663808822632\n",
      "Epoch 58/60, Batch 23/100, Loss: 0.19169442355632782\n",
      "Epoch 58/60, Batch 24/100, Loss: 0.09530804306268692\n",
      "Epoch 58/60, Batch 25/100, Loss: 0.06638722121715546\n",
      "Epoch 58/60, Batch 26/100, Loss: 0.07023207098245621\n",
      "Epoch 58/60, Batch 27/100, Loss: 0.1895570456981659\n",
      "Epoch 58/60, Batch 28/100, Loss: 0.08870184421539307\n",
      "Epoch 58/60, Batch 29/100, Loss: 0.13054101169109344\n",
      "Epoch 58/60, Batch 30/100, Loss: 0.05967876315116882\n",
      "Epoch 58/60, Batch 31/100, Loss: 0.05986601486802101\n",
      "Epoch 58/60, Batch 32/100, Loss: 0.03055911883711815\n",
      "Epoch 58/60, Batch 33/100, Loss: 0.054305050522089005\n",
      "Epoch 58/60, Batch 34/100, Loss: 0.06913483142852783\n",
      "Epoch 58/60, Batch 35/100, Loss: 0.034236058592796326\n",
      "Epoch 58/60, Batch 36/100, Loss: 0.08169203251600266\n",
      "Epoch 58/60, Batch 37/100, Loss: 0.04760121926665306\n",
      "Epoch 58/60, Batch 38/100, Loss: 0.03002159111201763\n",
      "Epoch 58/60, Batch 39/100, Loss: 0.039794281125068665\n",
      "Epoch 58/60, Batch 40/100, Loss: 0.055195923894643784\n",
      "Epoch 58/60, Batch 41/100, Loss: 0.15019409358501434\n",
      "Epoch 58/60, Batch 42/100, Loss: 0.03700236231088638\n",
      "Epoch 58/60, Batch 43/100, Loss: 0.03432769328355789\n",
      "Epoch 58/60, Batch 44/100, Loss: 0.057466134428977966\n",
      "Epoch 58/60, Batch 45/100, Loss: 0.03562849760055542\n",
      "Epoch 58/60, Batch 46/100, Loss: 0.04494030773639679\n",
      "Epoch 58/60, Batch 47/100, Loss: 0.024933021515607834\n",
      "Epoch 58/60, Batch 48/100, Loss: 0.09497284144163132\n",
      "Epoch 58/60, Batch 49/100, Loss: 0.05746850371360779\n",
      "Epoch 58/60, Batch 50/100, Loss: 0.028089549392461777\n",
      "Epoch 58/60, Batch 51/100, Loss: 0.02322915941476822\n",
      "Epoch 58/60, Batch 52/100, Loss: 0.07328815013170242\n",
      "Epoch 58/60, Batch 53/100, Loss: 0.055652618408203125\n",
      "Epoch 58/60, Batch 54/100, Loss: 0.14864805340766907\n",
      "Epoch 58/60, Batch 55/100, Loss: 0.048646166920661926\n",
      "Epoch 58/60, Batch 56/100, Loss: 0.03850370645523071\n",
      "Epoch 58/60, Batch 57/100, Loss: 0.012154880911111832\n",
      "Epoch 58/60, Batch 58/100, Loss: 0.030742423608899117\n",
      "Epoch 58/60, Batch 59/100, Loss: 0.024230172857642174\n",
      "Epoch 58/60, Batch 60/100, Loss: 0.0996934324502945\n",
      "Epoch 58/60, Batch 61/100, Loss: 0.17428694665431976\n",
      "Epoch 58/60, Batch 62/100, Loss: 0.061368390917778015\n",
      "Epoch 58/60, Batch 63/100, Loss: 0.04471202194690704\n",
      "Epoch 58/60, Batch 64/100, Loss: 0.05585392937064171\n",
      "Epoch 58/60, Batch 65/100, Loss: 0.03823595121502876\n",
      "Epoch 58/60, Batch 66/100, Loss: 0.085574671626091\n",
      "Epoch 58/60, Batch 67/100, Loss: 0.038903217762708664\n",
      "Epoch 58/60, Batch 68/100, Loss: 0.025863923132419586\n",
      "Epoch 58/60, Batch 69/100, Loss: 0.07498703896999359\n",
      "Epoch 58/60, Batch 70/100, Loss: 0.09518450498580933\n",
      "Epoch 58/60, Batch 71/100, Loss: 0.052925288677215576\n",
      "Epoch 58/60, Batch 72/100, Loss: 0.0566285103559494\n",
      "Epoch 58/60, Batch 73/100, Loss: 0.03370894864201546\n",
      "Epoch 58/60, Batch 74/100, Loss: 0.04404810070991516\n",
      "Epoch 58/60, Batch 75/100, Loss: 0.022251272574067116\n",
      "Epoch 58/60, Batch 76/100, Loss: 0.03023337572813034\n",
      "Epoch 58/60, Batch 77/100, Loss: 0.04883057624101639\n",
      "Epoch 58/60, Batch 78/100, Loss: 0.06399653851985931\n",
      "Epoch 58/60, Batch 79/100, Loss: 0.16857637465000153\n",
      "Epoch 58/60, Batch 80/100, Loss: 0.0638662725687027\n",
      "Epoch 58/60, Batch 81/100, Loss: 0.037961967289447784\n",
      "Epoch 58/60, Batch 82/100, Loss: 0.03146829828619957\n",
      "Epoch 58/60, Batch 83/100, Loss: 0.04332301393151283\n",
      "Epoch 58/60, Batch 84/100, Loss: 0.05721043050289154\n",
      "Epoch 58/60, Batch 85/100, Loss: 0.044661957770586014\n",
      "Epoch 58/60, Batch 86/100, Loss: 0.036144234240055084\n",
      "Epoch 58/60, Batch 87/100, Loss: 0.07066421210765839\n",
      "Epoch 58/60, Batch 88/100, Loss: 0.029700256884098053\n",
      "Epoch 58/60, Batch 89/100, Loss: 0.06173077970743179\n",
      "Epoch 58/60, Batch 90/100, Loss: 0.06723383069038391\n",
      "Epoch 58/60, Batch 91/100, Loss: 0.03500936180353165\n",
      "Epoch 58/60, Batch 92/100, Loss: 0.0936342179775238\n",
      "Epoch 58/60, Batch 93/100, Loss: 0.040824417024850845\n",
      "Epoch 58/60, Batch 94/100, Loss: 0.028615426272153854\n",
      "Epoch 58/60, Batch 95/100, Loss: 0.025096682831645012\n",
      "Epoch 58/60, Batch 96/100, Loss: 0.03437918797135353\n",
      "Epoch 58/60, Batch 97/100, Loss: 0.08716832101345062\n",
      "Epoch 58/60, Batch 98/100, Loss: 0.2701157033443451\n",
      "Epoch 58/60, Batch 99/100, Loss: 0.03396674990653992\n",
      "Epoch 58/60, Batch 100/100, Loss: 0.07125329971313477\n",
      "Saved model parameters to models/unet_epoch58.pdparams\n",
      "Epoch 59/60, Batch 1/100, Loss: 0.07923558354377747\n",
      "Epoch 59/60, Batch 2/100, Loss: 0.05118923634290695\n",
      "Epoch 59/60, Batch 3/100, Loss: 0.060703177005052567\n",
      "Epoch 59/60, Batch 4/100, Loss: 0.0582120306789875\n",
      "Epoch 59/60, Batch 5/100, Loss: 0.18325480818748474\n",
      "Epoch 59/60, Batch 6/100, Loss: 0.04752404987812042\n",
      "Epoch 59/60, Batch 7/100, Loss: 0.13430282473564148\n",
      "Epoch 59/60, Batch 8/100, Loss: 0.04389602690935135\n",
      "Epoch 59/60, Batch 9/100, Loss: 0.02818286046385765\n",
      "Epoch 59/60, Batch 10/100, Loss: 0.07046511769294739\n",
      "Epoch 59/60, Batch 11/100, Loss: 0.10293163359165192\n",
      "Epoch 59/60, Batch 12/100, Loss: 0.04473789036273956\n",
      "Epoch 59/60, Batch 13/100, Loss: 0.028025764971971512\n",
      "Epoch 59/60, Batch 14/100, Loss: 0.056798044592142105\n",
      "Epoch 59/60, Batch 15/100, Loss: 0.06792093068361282\n",
      "Epoch 59/60, Batch 16/100, Loss: 0.09300413727760315\n",
      "Epoch 59/60, Batch 17/100, Loss: 0.09859198331832886\n",
      "Epoch 59/60, Batch 18/100, Loss: 0.1000690758228302\n",
      "Epoch 59/60, Batch 19/100, Loss: 0.03303053602576256\n",
      "Epoch 59/60, Batch 20/100, Loss: 0.09714382886886597\n",
      "Epoch 59/60, Batch 21/100, Loss: 0.06762725859880447\n",
      "Epoch 59/60, Batch 22/100, Loss: 0.057018719613552094\n",
      "Epoch 59/60, Batch 23/100, Loss: 0.058298490941524506\n",
      "Epoch 59/60, Batch 24/100, Loss: 0.06496085971593857\n",
      "Epoch 59/60, Batch 25/100, Loss: 0.059297725558280945\n",
      "Epoch 59/60, Batch 26/100, Loss: 0.07125276327133179\n",
      "Epoch 59/60, Batch 27/100, Loss: 0.04757719486951828\n",
      "Epoch 59/60, Batch 28/100, Loss: 0.02450585551559925\n",
      "Epoch 59/60, Batch 29/100, Loss: 0.11768640577793121\n",
      "Epoch 59/60, Batch 30/100, Loss: 0.22050008177757263\n",
      "Epoch 59/60, Batch 31/100, Loss: 0.06623812019824982\n",
      "Epoch 59/60, Batch 32/100, Loss: 0.025992276147007942\n",
      "Epoch 59/60, Batch 33/100, Loss: 0.06016998738050461\n",
      "Epoch 59/60, Batch 34/100, Loss: 0.038649287074804306\n",
      "Epoch 59/60, Batch 35/100, Loss: 0.10471263527870178\n",
      "Epoch 59/60, Batch 36/100, Loss: 0.060951679944992065\n",
      "Epoch 59/60, Batch 37/100, Loss: 0.17955854535102844\n",
      "Epoch 59/60, Batch 38/100, Loss: 0.04984113574028015\n",
      "Epoch 59/60, Batch 39/100, Loss: 0.0619439035654068\n",
      "Epoch 59/60, Batch 40/100, Loss: 0.07530190795660019\n",
      "Epoch 59/60, Batch 41/100, Loss: 0.040938667953014374\n",
      "Epoch 59/60, Batch 42/100, Loss: 0.07536277174949646\n",
      "Epoch 59/60, Batch 43/100, Loss: 0.06768063455820084\n",
      "Epoch 59/60, Batch 44/100, Loss: 0.10885626077651978\n",
      "Epoch 59/60, Batch 45/100, Loss: 0.15208004415035248\n",
      "Epoch 59/60, Batch 46/100, Loss: 0.05566153675317764\n",
      "Epoch 59/60, Batch 47/100, Loss: 0.05218512937426567\n",
      "Epoch 59/60, Batch 48/100, Loss: 0.0472526028752327\n",
      "Epoch 59/60, Batch 49/100, Loss: 0.055387943983078\n",
      "Epoch 59/60, Batch 50/100, Loss: 0.04476174712181091\n",
      "Epoch 59/60, Batch 51/100, Loss: 0.039454489946365356\n",
      "Epoch 59/60, Batch 52/100, Loss: 0.04941197484731674\n",
      "Epoch 59/60, Batch 53/100, Loss: 0.04527263715863228\n",
      "Epoch 59/60, Batch 54/100, Loss: 0.07762067764997482\n",
      "Epoch 59/60, Batch 55/100, Loss: 0.05910239741206169\n",
      "Epoch 59/60, Batch 56/100, Loss: 0.07201866060495377\n",
      "Epoch 59/60, Batch 57/100, Loss: 0.047309402376413345\n",
      "Epoch 59/60, Batch 58/100, Loss: 0.05233456939458847\n",
      "Epoch 59/60, Batch 59/100, Loss: 0.04665307328104973\n",
      "Epoch 59/60, Batch 60/100, Loss: 0.033897969871759415\n",
      "Epoch 59/60, Batch 61/100, Loss: 0.04289668798446655\n",
      "Epoch 59/60, Batch 62/100, Loss: 0.06756353378295898\n",
      "Epoch 59/60, Batch 63/100, Loss: 0.06915028393268585\n",
      "Epoch 59/60, Batch 64/100, Loss: 0.019386708736419678\n",
      "Epoch 59/60, Batch 65/100, Loss: 0.06373895704746246\n",
      "Epoch 59/60, Batch 66/100, Loss: 0.19486364722251892\n",
      "Epoch 59/60, Batch 67/100, Loss: 0.050506483763456345\n",
      "Epoch 59/60, Batch 68/100, Loss: 0.060933638364076614\n",
      "Epoch 59/60, Batch 69/100, Loss: 0.023679548874497414\n",
      "Epoch 59/60, Batch 70/100, Loss: 0.03427674248814583\n",
      "Epoch 59/60, Batch 71/100, Loss: 0.027100928127765656\n",
      "Epoch 59/60, Batch 72/100, Loss: 0.0525524728000164\n",
      "Epoch 59/60, Batch 73/100, Loss: 0.07299400866031647\n",
      "Epoch 59/60, Batch 74/100, Loss: 0.02069983445107937\n",
      "Epoch 59/60, Batch 75/100, Loss: 0.07319281995296478\n",
      "Epoch 59/60, Batch 76/100, Loss: 0.07004851847887039\n",
      "Epoch 59/60, Batch 77/100, Loss: 0.0833907425403595\n",
      "Epoch 59/60, Batch 78/100, Loss: 0.04804196208715439\n",
      "Epoch 59/60, Batch 79/100, Loss: 0.11194866895675659\n",
      "Epoch 59/60, Batch 80/100, Loss: 0.035274676978588104\n",
      "Epoch 59/60, Batch 81/100, Loss: 0.031549789011478424\n",
      "Epoch 59/60, Batch 82/100, Loss: 0.06963780522346497\n",
      "Epoch 59/60, Batch 83/100, Loss: 0.07965816557407379\n",
      "Epoch 59/60, Batch 84/100, Loss: 0.08116050064563751\n",
      "Epoch 59/60, Batch 85/100, Loss: 0.059648409485816956\n",
      "Epoch 59/60, Batch 86/100, Loss: 0.06823714077472687\n",
      "Epoch 59/60, Batch 87/100, Loss: 0.0736449807882309\n",
      "Epoch 59/60, Batch 88/100, Loss: 0.137445867061615\n",
      "Epoch 59/60, Batch 89/100, Loss: 0.07097933441400528\n",
      "Epoch 59/60, Batch 90/100, Loss: 0.04856771230697632\n",
      "Epoch 59/60, Batch 91/100, Loss: 0.09160598367452621\n",
      "Epoch 59/60, Batch 92/100, Loss: 0.03857922554016113\n",
      "Epoch 59/60, Batch 93/100, Loss: 0.036609649658203125\n",
      "Epoch 59/60, Batch 94/100, Loss: 0.06056826189160347\n",
      "Epoch 59/60, Batch 95/100, Loss: 0.047419723123311996\n",
      "Epoch 59/60, Batch 96/100, Loss: 0.05768171325325966\n",
      "Epoch 59/60, Batch 97/100, Loss: 0.0677521601319313\n",
      "Epoch 59/60, Batch 98/100, Loss: 0.02681061252951622\n",
      "Epoch 59/60, Batch 99/100, Loss: 0.04193183407187462\n",
      "Epoch 59/60, Batch 100/100, Loss: 0.0342232771217823\n",
      "Saved model parameters to models/unet_epoch59.pdparams\n",
      "Epoch 60/60, Batch 1/100, Loss: 0.07859287410974503\n",
      "Epoch 60/60, Batch 2/100, Loss: 0.06940588355064392\n",
      "Epoch 60/60, Batch 3/100, Loss: 0.03752294182777405\n",
      "Epoch 60/60, Batch 4/100, Loss: 0.042774755507707596\n",
      "Epoch 60/60, Batch 5/100, Loss: 0.032895468175411224\n",
      "Epoch 60/60, Batch 6/100, Loss: 0.20832903683185577\n",
      "Epoch 60/60, Batch 7/100, Loss: 0.0347992479801178\n",
      "Epoch 60/60, Batch 8/100, Loss: 0.09212756901979446\n",
      "Epoch 60/60, Batch 9/100, Loss: 0.05233899503946304\n",
      "Epoch 60/60, Batch 10/100, Loss: 0.1355113685131073\n",
      "Epoch 60/60, Batch 11/100, Loss: 0.10408277064561844\n",
      "Epoch 60/60, Batch 12/100, Loss: 0.023506322875618935\n",
      "Epoch 60/60, Batch 13/100, Loss: 0.04142143949866295\n",
      "Epoch 60/60, Batch 14/100, Loss: 0.03499714285135269\n",
      "Epoch 60/60, Batch 15/100, Loss: 0.1255279779434204\n",
      "Epoch 60/60, Batch 16/100, Loss: 0.05376512184739113\n",
      "Epoch 60/60, Batch 17/100, Loss: 0.08115518093109131\n",
      "Epoch 60/60, Batch 18/100, Loss: 0.06954850256443024\n",
      "Epoch 60/60, Batch 19/100, Loss: 0.14803268015384674\n",
      "Epoch 60/60, Batch 20/100, Loss: 0.060586102306842804\n",
      "Epoch 60/60, Batch 21/100, Loss: 0.0585341714322567\n",
      "Epoch 60/60, Batch 22/100, Loss: 0.13816939294338226\n",
      "Epoch 60/60, Batch 23/100, Loss: 0.05830453708767891\n",
      "Epoch 60/60, Batch 24/100, Loss: 0.027535144239664078\n",
      "Epoch 60/60, Batch 25/100, Loss: 0.051110029220581055\n",
      "Epoch 60/60, Batch 26/100, Loss: 0.03342205658555031\n",
      "Epoch 60/60, Batch 27/100, Loss: 0.05151930823922157\n",
      "Epoch 60/60, Batch 28/100, Loss: 0.028963657096028328\n",
      "Epoch 60/60, Batch 29/100, Loss: 0.06976456940174103\n",
      "Epoch 60/60, Batch 30/100, Loss: 0.15751267969608307\n",
      "Epoch 60/60, Batch 31/100, Loss: 0.03600538894534111\n",
      "Epoch 60/60, Batch 32/100, Loss: 0.057020947337150574\n",
      "Epoch 60/60, Batch 33/100, Loss: 0.09095200896263123\n",
      "Epoch 60/60, Batch 34/100, Loss: 0.0708526223897934\n",
      "Epoch 60/60, Batch 35/100, Loss: 0.04160510376095772\n",
      "Epoch 60/60, Batch 36/100, Loss: 0.07057429850101471\n",
      "Epoch 60/60, Batch 37/100, Loss: 0.029801564291119576\n",
      "Epoch 60/60, Batch 38/100, Loss: 0.05681493133306503\n",
      "Epoch 60/60, Batch 39/100, Loss: 0.05196705833077431\n",
      "Epoch 60/60, Batch 40/100, Loss: 0.028362462297081947\n",
      "Epoch 60/60, Batch 41/100, Loss: 0.07047362625598907\n",
      "Epoch 60/60, Batch 42/100, Loss: 0.022809362038969994\n",
      "Epoch 60/60, Batch 43/100, Loss: 0.11836998909711838\n",
      "Epoch 60/60, Batch 44/100, Loss: 0.038443151861429214\n",
      "Epoch 60/60, Batch 45/100, Loss: 0.028287513181567192\n",
      "Epoch 60/60, Batch 46/100, Loss: 0.012327250093221664\n",
      "Epoch 60/60, Batch 47/100, Loss: 0.0747508555650711\n",
      "Epoch 60/60, Batch 48/100, Loss: 0.08091123402118683\n",
      "Epoch 60/60, Batch 49/100, Loss: 0.042314156889915466\n",
      "Epoch 60/60, Batch 50/100, Loss: 0.051512427628040314\n",
      "Epoch 60/60, Batch 51/100, Loss: 0.03488374873995781\n",
      "Epoch 60/60, Batch 52/100, Loss: 0.05393664166331291\n",
      "Epoch 60/60, Batch 53/100, Loss: 0.0948810800909996\n",
      "Epoch 60/60, Batch 54/100, Loss: 0.05592694878578186\n",
      "Epoch 60/60, Batch 55/100, Loss: 0.03358311206102371\n",
      "Epoch 60/60, Batch 56/100, Loss: 0.043336961418390274\n",
      "Epoch 60/60, Batch 57/100, Loss: 0.05520108342170715\n",
      "Epoch 60/60, Batch 58/100, Loss: 0.0421467088162899\n",
      "Epoch 60/60, Batch 59/100, Loss: 0.07424827665090561\n",
      "Epoch 60/60, Batch 60/100, Loss: 0.05991845577955246\n",
      "Epoch 60/60, Batch 61/100, Loss: 0.025046929717063904\n",
      "Epoch 60/60, Batch 62/100, Loss: 0.04688384011387825\n",
      "Epoch 60/60, Batch 63/100, Loss: 0.06023159995675087\n",
      "Epoch 60/60, Batch 64/100, Loss: 0.016463657841086388\n",
      "Epoch 60/60, Batch 65/100, Loss: 0.02668282389640808\n",
      "Epoch 60/60, Batch 66/100, Loss: 0.021025709807872772\n",
      "Epoch 60/60, Batch 67/100, Loss: 0.06297139078378677\n",
      "Epoch 60/60, Batch 68/100, Loss: 0.024407295510172844\n",
      "Epoch 60/60, Batch 69/100, Loss: 0.04024771973490715\n",
      "Epoch 60/60, Batch 70/100, Loss: 0.053640030324459076\n",
      "Epoch 60/60, Batch 71/100, Loss: 0.03139959275722504\n",
      "Epoch 60/60, Batch 72/100, Loss: 0.03518182039260864\n",
      "Epoch 60/60, Batch 73/100, Loss: 0.024948084726929665\n",
      "Epoch 60/60, Batch 74/100, Loss: 0.03918444737792015\n",
      "Epoch 60/60, Batch 75/100, Loss: 0.04150954261422157\n",
      "Epoch 60/60, Batch 76/100, Loss: 0.016852660104632378\n",
      "Epoch 60/60, Batch 77/100, Loss: 0.056225065141916275\n",
      "Epoch 60/60, Batch 78/100, Loss: 0.02049163542687893\n",
      "Epoch 60/60, Batch 79/100, Loss: 0.10240299999713898\n",
      "Epoch 60/60, Batch 80/100, Loss: 0.10708687454462051\n",
      "Epoch 60/60, Batch 81/100, Loss: 0.04602660611271858\n",
      "Epoch 60/60, Batch 82/100, Loss: 0.06744077801704407\n",
      "Epoch 60/60, Batch 83/100, Loss: 0.0321413055062294\n",
      "Epoch 60/60, Batch 84/100, Loss: 0.062825046479702\n",
      "Epoch 60/60, Batch 85/100, Loss: 0.022651147097349167\n",
      "Epoch 60/60, Batch 86/100, Loss: 0.0632941946387291\n",
      "Epoch 60/60, Batch 87/100, Loss: 0.026884939521551132\n",
      "Epoch 60/60, Batch 88/100, Loss: 0.027383772656321526\n",
      "Epoch 60/60, Batch 89/100, Loss: 0.02037670649588108\n",
      "Epoch 60/60, Batch 90/100, Loss: 0.07087540626525879\n",
      "Epoch 60/60, Batch 91/100, Loss: 0.04454079642891884\n",
      "Epoch 60/60, Batch 92/100, Loss: 0.014917416498064995\n",
      "Epoch 60/60, Batch 93/100, Loss: 0.060479599982500076\n",
      "Epoch 60/60, Batch 94/100, Loss: 0.02644364908337593\n",
      "Epoch 60/60, Batch 95/100, Loss: 0.029318997636437416\n",
      "Epoch 60/60, Batch 96/100, Loss: 0.0647001639008522\n",
      "Epoch 60/60, Batch 97/100, Loss: 0.03355436772108078\n",
      "Epoch 60/60, Batch 98/100, Loss: 0.030249333009123802\n",
      "Epoch 60/60, Batch 99/100, Loss: 0.04765743017196655\n",
      "Epoch 60/60, Batch 100/100, Loss: 0.05385041981935501\n",
      "Saved model parameters to models/unet_epoch60.pdparams\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# 1. 初始化模型\n",
    "model = UNet(num_classes=1)\n",
    "\n",
    "# 2. 加载模型参数\n",
    "start_epoch = 50\n",
    "model_path = os.path.join(\"models\", f\"unet_epoch{start_epoch}.pdparams\")\n",
    "model_state_dict = paddle.load(model_path)\n",
    "model.set_state_dict(model_state_dict)\n",
    "print(f\"Loaded model parameters from {model_path}\")\n",
    "\n",
    "# 3. 设置损失函数和优化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(parameters=model.parameters(), learning_rate=0.001)\n",
    "\n",
    "# 创建一个LogWriter对象\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "log_dir = f\"vdl_logs/{current_time}_epochs_{epochs}_from_epoch_{start_epoch}\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "writer = LogWriter(log_dir)\n",
    "\n",
    "# 4. 继续训练\n",
    "epochs = 60\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        total_loss += loss.numpy()[0]\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.numpy()[0]}\")\n",
    "\n",
    "    # 记录平均loss到VisualDL\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar(tag=\"train/avg_loss\", step=epoch, value=avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss}\")\n",
    "\n",
    "    # 挑选一个测试图片放入VDL\n",
    "    test_image, test_mask = next(iter(test_loader))\n",
    "    test_output = model(test_image)\n",
    "    writer.add_image(tag=\"test/image\", img=test_image[0], step=epoch)\n",
    "    writer.add_image(tag=\"test/prediction\", img=test_output[0], step=epoch)\n",
    "    writer.add_image(tag=\"test/mask\", img=test_mask[0], step=epoch)\n",
    "\n",
    "    # 每个epoch结束后保存模型\n",
    "    model_path = os.path.join(\"Unet_models\", f\"unet_epoch{epoch+1}.pdparams\")\n",
    "    paddle.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model parameters to {model_path}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fd70a-eb12-42ea-96f6-55a2cb44606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    total_loss = 0.0\n",
    "    with paddle.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.numpy()[0]\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def test(model, dataloader, save_dir=\"predictions\"):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    with paddle.no_grad():\n",
    "        for idx, (images, _) in enumerate(dataloader):\n",
    "            outputs = model(images)\n",
    "            # 将输出转换为二值掩码\n",
    "            predicted_masks = (outputs > 0.5).astype('float32')\n",
    "            for i, mask in enumerate(predicted_masks):\n",
    "                mask_img = Image.fromarray((mask[0].numpy() * 255).astype(np.uint8))\n",
    "                mask_img.save(os.path.join(save_dir, f\"predicted_mask_{idx * len(images) + i}.bmp\"))\n",
    "\n",
    "# 加载模型参数\n",
    "model_path = \"path_to_your_saved_model.pdparams\"\n",
    "model_state_dict = paddle.load(model_path)\n",
    "model.set_state_dict(model_state_dict)\n",
    "\n",
    "# 创建验证集和测试集的DataLoader\n",
    "val_dataset = EyeDataset(val_image_dir, val_mask_dir, transform_size=(512,512))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "test_dataset = EyeDataset(test_image_dir, test_mask_dir, transform_size=(512,512))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 评估模型\n",
    "val_loss = evaluate(model, criterion, val_loader)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "# 测试模型\n",
    "test(model, test_loader)\n",
    "print(\"Testing completed and predictions saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aef71a-52fa-43b6-97d9-4b53afeb6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试单张\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def predict_single_image(model, image_path, transform_size=(512, 512)):\n",
    "    # 加载和预处理图像\n",
    "    image = Image.open(image_path).convert('RGB').resize(transform_size)\n",
    "    image_tensor = paddle.to_tensor(np.array(image).astype('float32').transpose((2, 0, 1)) / 255.0)\n",
    "    image_tensor = paddle.unsqueeze(image_tensor, 0)  # 添加batch维度\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    model.eval()\n",
    "    with paddle.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        predicted_mask = (output > 0.5).astype('float32')\n",
    "\n",
    "    # 将预测的掩码转换回图像格式\n",
    "    mask_img = Image.fromarray((predicted_mask[0][0].numpy() * 255).astype(np.uint8)).resize((2000, 2000))\n",
    "    \n",
    "    return mask_img\n",
    "model = UNet(num_classes=1)\n",
    "# 加载模型参数\n",
    "model_path = \"Unet_models/unet_epoch50.pdparams\"\n",
    "model_state_dict = paddle.load(model_path)\n",
    "model.set_state_dict(model_state_dict)\n",
    "\n",
    "# 预测单张图像\n",
    "image_path = \"V0017.jpg\"\n",
    "predicted_mask_img = predict_single_image(model, image_path)\n",
    "\n",
    "# 展示预测的掩码\n",
    "plt.imshow(predicted_mask_img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# 保存预测的掩码\n",
    "predicted_mask_img.save(\"V0017.bmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc7472-8d5f-4056-bda4-a5fe9785f757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05981c57-4b4f-45a0-ae62-dbc7db554d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bad72-46dc-4bb3-accd-597387321c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd0703-ba5f-4aba-8c19-05ddf27829ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258cf1c-5442-4ed7-8740-c200f3a94535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d38aec-c616-4751-b2e0-47fab1f42d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
